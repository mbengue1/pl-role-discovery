{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c4ed8a",
   "metadata": {},
   "source": [
    "# Premier League Player Role Discovery - Phase 2: Feature Engineering\n",
    "\n",
    "This notebook implements the feature engineering phase for the Premier League Player Role Discovery project. It builds on the cleaned dataset from Phase 1 and creates:\n",
    "\n",
    "1. Per-90 normalized versions of relevant statistics\n",
    "2. Composite indices (PI, CCI, DA, FE)\n",
    "3. Winsorized features to handle outliers\n",
    "\n",
    "The final engineered dataset will be saved for use in subsequent clustering and analysis phases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969913e8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3de796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Set, Union\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set display options for better dataframe viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf09935",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da9b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "def calculate_composite_index(df: pd.DataFrame, features: List[str], \n",
    "                             weights: List[float], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate a composite index from multiple features using weights\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe with player statistics\n",
    "        features: list of feature names to include in the index\n",
    "        weights: list of weights for each feature (must sum to 1)\n",
    "        name: name of the composite index\n",
    "        \n",
    "    Returns:\n",
    "        series with the calculated composite index\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(features) != len(weights):\n",
    "        raise ValueError(\"Features and weights must have the same length\")\n",
    "    \n",
    "    if abs(sum(weights) - 1.0) > 0.001:\n",
    "        raise ValueError(\"Weights must sum to 1.0\")\n",
    "    \n",
    "    # Check if all features exist in the dataframe\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Features {missing_features} not found in dataframe\")\n",
    "    \n",
    "    # Create a copy of the features for standardization\n",
    "    features_df = df[features].copy()\n",
    "    \n",
    "    # Standardize each feature\n",
    "    scaler = StandardScaler()\n",
    "    features_standardized = scaler.fit_transform(features_df)\n",
    "    \n",
    "    # Convert back to dataframe for easier handling\n",
    "    features_std_df = pd.DataFrame(features_standardized, columns=features, index=df.index)\n",
    "    \n",
    "    # Calculate weighted sum\n",
    "    composite_index = pd.Series(0, index=df.index)\n",
    "    for feature, weight in zip(features, weights):\n",
    "        composite_index += features_std_df[feature] * weight\n",
    "    \n",
    "    return composite_index\n",
    "\n",
    "def winsorize_df(df: pd.DataFrame, lower: float = 0.05, upper: float = 0.95,\n",
    "                exclude_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply winsorization to handle outliers in numeric columns\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe with player statistics\n",
    "        lower: lower percentile for winsorization (default: 0.05)\n",
    "        upper: upper percentile for winsorization (default: 0.95)\n",
    "        exclude_cols: list of columns to exclude from winsorization\n",
    "        \n",
    "    Returns:\n",
    "        dataframe with winsorized values\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Default exclude columns if none provided\n",
    "    if exclude_cols is None:\n",
    "        # Use a default list of common ID columns instead of global variable\n",
    "        default_exclude = ['player', 'team', 'position', 'data_source', 'nation']\n",
    "        exclude_cols = default_exclude\n",
    "    \n",
    "    # Get numeric columns that are not in exclude_cols\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    cols_to_winsorize = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Track columns that were winsorized\n",
    "    winsorized_count = 0\n",
    "    \n",
    "    # Winsorize each column\n",
    "    for col in cols_to_winsorize:\n",
    "        # Skip columns with all NaN values\n",
    "        if result_df[col].isna().all():\n",
    "            continue\n",
    "        \n",
    "        try:    \n",
    "            # Winsorize the column\n",
    "            result_df[col] = stats.mstats.winsorize(result_df[col], limits=[lower, 1-upper])\n",
    "            winsorized_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not winsorize column '{col}': {str(e)}\")\n",
    "    \n",
    "    print(f\"Winsorized {winsorized_count} columns\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be731e3f",
   "metadata": {},
   "source": [
    "## 3. Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619272b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "INPUT_FILE = '../data/processed/player_stats_cleaned.csv'\n",
    "OUTPUT_FILE = '../data/processed/player_stats_engineered.csv'\n",
    "METADATA_FILE = '../data/processed/feature_metadata.json'\n",
    "\n",
    "# define constants for feature engineering\n",
    "WINSOR_LOWER = 0.05  # 5th percentile\n",
    "WINSOR_UPPER = 0.95  # 95th percentile\n",
    "\n",
    "# columns that should not be normalized or winsorized\n",
    "ID_COLUMNS = ['player', 'team', 'position', 'data_source', 'nation']\n",
    "\n",
    "# columns that should not be converted to numeric (including columns with mixed data types)\n",
    "NON_NUMERIC_COLUMNS = ID_COLUMNS + [\n",
    "    'nation_misc', 'position_misc', 'team_misc',\n",
    "    'nation_passing', 'position_passing', 'team_passing',\n",
    "    'nation_playtime', 'position_playtime', 'team_playtime',\n",
    "    'nation_defense', 'position_defense', 'team_defense',\n",
    "    'nation_pass_types', 'position_pass_types', 'team_pass_types',\n",
    "    'nation_creation', 'position_creation', 'team_creation',\n",
    "    'nation_possession', 'position_possession', 'team_possession',\n",
    "    'nation_shooting', 'position_shooting', 'team_shooting'\n",
    "]\n",
    "\n",
    "# define composite index components and weights\n",
    "COMPOSITE_INDICES = {\n",
    "    'PI': {  # Progressive Index\n",
    "        'components': ['prgp_per90', 'prgc_per90', 'succ_per90'],\n",
    "        'weights': [0.4, 0.4, 0.2]  # progressive passes and carries weighted more\n",
    "    },\n",
    "    'CCI': {  # Creative Contribution Index\n",
    "        'components': ['kp_per90', 'sca_per90', 'xag_per90'],\n",
    "        'weights': [0.3, 0.4, 0.3]  # equal weights with slight emphasis on shot-creating actions\n",
    "    },\n",
    "    'DA': {  # Defensive Activity Index\n",
    "        'components': ['tkl_per90', 'int_per90', 'pressures_per90', 'blocks_per90'],\n",
    "        'weights': [0.3, 0.3, 0.2, 0.2]  # tackles and interceptions weighted more\n",
    "    },\n",
    "    'FE': {  # Final Execution Index\n",
    "        'components': ['gls_per90', 'xg_per90', 'sot_per90', 'g_per_sh'],\n",
    "        'weights': [0.3, 0.3, 0.2, 0.2]  # goals and xG weighted more\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a89d78",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83a6e1",
   "metadata": {},
   "source": [
    "### Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4d4582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ../data/processed/player_stats_cleaned.csv\n",
      "loaded 563 rows and 248 columns\n"
     ]
    }
   ],
   "source": [
    "# load the cleaned dataset\n",
    "print(f\"loading data from {INPUT_FILE}\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"error loading data: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5017e",
   "metadata": {},
   "source": [
    "### Check for Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c0d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 duplicate rows\n",
      "\n",
      "missing values in key fields:\n",
      "player      0\n",
      "team        0\n",
      "position    0\n",
      "minutes     0\n",
      "dtype: int64\n",
      "\n",
      "dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 563 entries, 0 to 562\n",
      "Columns: 248 entries, player to data_source\n",
      "dtypes: int64(9), object(239)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"found {duplicate_count} duplicate rows\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"removing duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"dataset now has {df.shape[0]} rows\")\n",
    "\n",
    "# check for missing values in key fields\n",
    "key_fields = ['player', 'team', 'position', 'minutes']\n",
    "missing_key_fields = df[key_fields].isna().sum()\n",
    "print(\"\\nmissing values in key fields:\")\n",
    "print(missing_key_fields)\n",
    "\n",
    "if missing_key_fields.sum() > 0:\n",
    "    print(\"removing rows with missing key fields...\")\n",
    "    df = df.dropna(subset=key_fields)\n",
    "    print(f\"dataset now has {df.shape[0]} rows\")\n",
    "\n",
    "# store original column names for metadata\n",
    "original_columns = df.columns.tolist()\n",
    "\n",
    "# display basic info about the dataset\n",
    "print(\"\\ndataset info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d8ef6",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606fbfd",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46031524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting string columns to numeric...\n",
      "\n",
      "Sample of column content before conversion:\n",
      "gls: ['0', '0', '2']\n",
      "xg: ['0.2', '0', '1.2']\n",
      "prgp: ['36', '0', '149']\n",
      "tkl: ['11', '0', '70']\n",
      "minutes: ['824', '2,700', '3,154']\n",
      "\n",
      "Sample of identifier columns to preserve:\n",
      "nation_misc: ['eng ENG', 'eng ENG', 'cd COD']\n",
      "position_misc: ['DF', 'GK', 'DF']\n",
      "team_misc: ['West Ham', 'Southampton', 'West Ham']\n",
      "\n",
      "Numeric columns before conversion: 9\n",
      "Numeric columns after conversion: 219\n",
      "Converted 210 columns to numeric\n",
      "Successful conversions: 210\n",
      "\n",
      "Verifying identifier columns are preserved:\n",
      "nation_misc: ['eng ENG', 'eng ENG', 'cd COD']\n",
      "position_misc: ['DF', 'GK', 'DF']\n",
      "team_misc: ['West Ham', 'Southampton', 'West Ham']\n"
     ]
    }
   ],
   "source": [
    "# Check data types and convert string columns to numeric where appropriate\n",
    "print(\"Converting string columns to numeric...\")\n",
    "\n",
    "# Sample a few columns to see their content\n",
    "print(\"\\nSample of column content before conversion:\")\n",
    "for col in ['gls', 'xg', 'prgp', 'tkl', 'minutes']:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].head(3).tolist()}\")\n",
    "\n",
    "# Sample a few identifier columns we want to preserve\n",
    "print(\"\\nSample of identifier columns to preserve:\")\n",
    "for col in ['nation_misc', 'position_misc', 'team_misc'][:3]:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].head(3).tolist()}\")\n",
    "\n",
    "# Count numeric columns before conversion\n",
    "numeric_count_before = len(df.select_dtypes(include=[np.number]).columns)\n",
    "print(f\"\\nNumeric columns before conversion: {numeric_count_before}\")\n",
    "\n",
    "# Track conversion issues\n",
    "conversion_issues = []\n",
    "conversion_success = 0\n",
    "\n",
    "# Convert string columns to numeric, excluding identifier columns\n",
    "for col in df.columns:\n",
    "    if col not in NON_NUMERIC_COLUMNS and df[col].dtype == 'object':\n",
    "        try:\n",
    "            # Try to convert to numeric, handling commas and other formatting\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            conversion_success += 1\n",
    "        except ValueError as e:\n",
    "            # Handle value errors specifically\n",
    "            conversion_issues.append(f\"Value error in column '{col}': {str(e)}\")\n",
    "        except TypeError as e:\n",
    "            # Handle type errors specifically\n",
    "            conversion_issues.append(f\"Type error in column '{col}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            # Catch other unexpected errors\n",
    "            conversion_issues.append(f\"Unexpected error in column '{col}': {str(e)}\")\n",
    "\n",
    "# Count numeric columns after conversion\n",
    "numeric_count_after = len(df.select_dtypes(include=[np.number]).columns)\n",
    "print(f\"Numeric columns after conversion: {numeric_count_after}\")\n",
    "print(f\"Converted {numeric_count_after - numeric_count_before} columns to numeric\")\n",
    "print(f\"Successful conversions: {conversion_success}\")\n",
    "\n",
    "# Print conversion issues (if any)\n",
    "if conversion_issues:\n",
    "    print(f\"\\nEncountered {len(conversion_issues)} conversion issues:\")\n",
    "    for issue in conversion_issues[:5]:  # Show first 5 issues\n",
    "        print(f\"  - {issue}\")\n",
    "    if len(conversion_issues) > 5:\n",
    "        print(f\"  ... and {len(conversion_issues) - 5} more issues\")\n",
    "\n",
    "# Verify that identifier columns were preserved\n",
    "print(\"\\nVerifying identifier columns are preserved:\")\n",
    "for col in ['nation_misc', 'position_misc', 'team_misc'][:3]:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].head(3).tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1e27f",
   "metadata": {},
   "source": [
    "## 6. Analyze Column Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89677ea9",
   "metadata": {},
   "source": [
    "### Analyze Column Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ef377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing column names...\n",
      "\n",
      "Progressive columns found (7):\n",
      "  - carries\n",
      "  - prgc\n",
      "  - prgc_possession\n",
      "  - prgp\n",
      "  - prgp_passing\n",
      "  - succ\n",
      "  - succpct\n",
      "\n",
      "Creative columns found (18):\n",
      "  - a_xag\n",
      "  - a_xag\n",
      "  - kp\n",
      "  - npxgplusxag\n",
      "  - npxgplusxag\n",
      "  - npxgplusxag1\n",
      "  - npxgplusxag1\n",
      "  - sca\n",
      "  - sca90\n",
      "  - xa\n",
      "  ... and 8 more\n",
      "\n",
      "Defensive columns found (30):\n",
      "  - 90s_defense\n",
      "  - age_defense\n",
      "  - att_defense\n",
      "  - blocks\n",
      "  - blocks_pass_types\n",
      "  - born_defense\n",
      "  - def\n",
      "  - def1\n",
      "  - def_3rd\n",
      "  - def_3rd_possession\n",
      "  ... and 20 more\n",
      "\n",
      "Finishing columns found (23):\n",
      "  - g_per_sot\n",
      "  - g_xg\n",
      "  - gls\n",
      "  - gls1\n",
      "  - gls_shooting\n",
      "  - np:g_xg\n",
      "  - npxg\n",
      "  - npxg1\n",
      "  - npxg_per_sh\n",
      "  - npxg_shooting\n",
      "  ... and 13 more\n",
      "\n",
      "Creating improved column mappings based on found columns...\n"
     ]
    }
   ],
   "source": [
    "# todo:examine the column names to better understand what's available\n",
    "print(\"Analyzing column names...\")\n",
    "\n",
    "# Track conversion issues\n",
    "column_conversion_issues = []\n",
    "\n",
    "# Convert columns to numeric where appropriate, preserving identifier columns\n",
    "for col in df.columns:\n",
    "    if col not in NON_NUMERIC_COLUMNS and df[col].dtype == 'object':\n",
    "        try:\n",
    "            # Try to convert to numeric, handling commas and other formatting\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                errors='coerce'\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            # Handle value errors specifically\n",
    "            column_conversion_issues.append(f\"Value error in column '{col}': {str(e)}\")\n",
    "        except TypeError as e:\n",
    "            # Handle type errors specifically\n",
    "            column_conversion_issues.append(f\"Type error in column '{col}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            # Catch other unexpected errors\n",
    "            column_conversion_issues.append(f\"Unexpected error in column '{col}': {str(e)}\")\n",
    "\n",
    "# Print conversion issues if any occurred\n",
    "if column_conversion_issues:\n",
    "    print(f\"Note: {len(column_conversion_issues)} conversion issues encountered during column analysis\")\n",
    "\n",
    "# Check for key stat columns we need for our indices\n",
    "key_stats = {\n",
    "    'Progressive': ['prgp', 'prgc', 'prog', 'succ', 'carries', 'dribble'],\n",
    "    'Creative': ['kp', 'sca', 'xag', 'xa', 'assist', 'key'],\n",
    "    'Defensive': ['tkl', 'int', 'press', 'block', 'def', 'tack'],\n",
    "    'Finishing': ['gls', 'xg', 'sot', 'shot', 'goal', 'finish']\n",
    "}\n",
    "\n",
    "# Search for columns matching our key stats\n",
    "found_columns = {}\n",
    "for category, terms in key_stats.items():\n",
    "    found_columns[category] = []\n",
    "    for term in terms:\n",
    "        matching_cols = [col for col in df.columns if term.lower() in col.lower()]\n",
    "        if matching_cols:\n",
    "            found_columns[category].extend(matching_cols)\n",
    "\n",
    "# Print the results\n",
    "for category, cols in found_columns.items():\n",
    "    print(f\"\\n{category} columns found ({len(cols)}):\")\n",
    "    for col in sorted(cols)[:10]:  # Show first 10 to avoid too much output\n",
    "        print(f\"  - {col}\")\n",
    "    if len(cols) > 10:\n",
    "        print(f\"  ... and {len(cols) - 10} more\")\n",
    "\n",
    "# Create improved column mappings based on what we found\n",
    "print(\"\\nCreating improved column mappings based on found columns...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed74bb8",
   "metadata": {},
   "source": [
    "### Improved Column Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82808f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Populating components for PI:\n",
      "  Found passes: prgp\n",
      "  Found carries: prgc\n",
      "  Found dribbles: succ\n",
      "  Total components found: 3/3\n",
      "\n",
      "Populating components for CCI:\n",
      "  Found key_passes: kp\n",
      "  Found shot_creation: sca\n",
      "  Found expected_assists: xag\n",
      "  Total components found: 3/3\n",
      "\n",
      "Populating components for DA:\n",
      "  Found tackles: tkl\n",
      "  Found interceptions: int\n",
      "  Found pressures: recov\n",
      "  Found blocks: blocks\n",
      "  Total components found: 4/4\n",
      "\n",
      "Populating components for FE:\n",
      "  Found goals: gls\n",
      "  Found expected_goals: xg\n",
      "  Found shots_on_target: sot\n",
      "  Found conversion: g_per_sh\n",
      "  Total components found: 4/4\n",
      "Updated PI with 3 components\n",
      "Updated CCI with 3 components\n",
      "Updated DA with 4 components\n",
      "Updated FE with 4 components\n"
     ]
    }
   ],
   "source": [
    "# Create improved column mappings based on what we found\n",
    "# We'll create per90 versions of these columns in the next step\n",
    "\n",
    "# Define our composite indices with more flexible column options\n",
    "IMPROVED_INDICES = {\n",
    "    'PI': {  # Progressive Index\n",
    "        'components': [],  # Will be populated based on found columns\n",
    "        'weight_groups': {\n",
    "            'passes': ['prgp', 'prgp_passing', 'prgp_possession'],  # 40%\n",
    "            'carries': ['prgc', 'prgc_possession', 'carries'],      # 40%\n",
    "            'dribbles': ['succ', 'succ_possession', 'dribble']      # 20%\n",
    "        },\n",
    "        'group_weights': [0.4, 0.4, 0.2]\n",
    "    },\n",
    "    'CCI': {  # Creative Contribution Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'key_passes': ['kp', 'kp_passing', '1_per_3'],          # 30%\n",
    "            'shot_creation': ['sca', 'sca_creation', 'sca90'],      # 40%\n",
    "            'expected_assists': ['xag', 'xag_passing', 'xa', 'a_xag'] # 30%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.4, 0.3]\n",
    "    },\n",
    "    'DA': {  # Defensive Activity Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'tackles': ['tkl', 'tkl_defense', 'tklw', 'tklw_defense'],  # 30%\n",
    "            'interceptions': ['int', 'int_defense'],                    # 30%\n",
    "            'pressures': ['press', 'pres', 'recov'],                    # 20%\n",
    "            'blocks': ['blocks', 'blocks_defense', 'blocks_pass_types'] # 20%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.3, 0.2, 0.2]\n",
    "    },\n",
    "    'FE': {  # Final Execution Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'goals': ['gls', 'gls_shooting', 'gls1'],                # 30%\n",
    "            'expected_goals': ['xg', 'xg_shooting', 'xg1', 'npxg'],  # 30%\n",
    "            'shots_on_target': ['sot', 'sot_shooting', 'sh_shooting'], # 20%\n",
    "            'conversion': ['g_per_sh', 'g_per_sot', 'g_xg']          # 20%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.3, 0.2, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# For each index, find the best matching column for each component\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    print(f\"\\nPopulating components for {index_name}:\")\n",
    "    \n",
    "    # For each weight group, find the best matching column\n",
    "    for group_name, search_terms in index_info['weight_groups'].items():\n",
    "        best_match = None\n",
    "        \n",
    "        # Try each search term in order of preference\n",
    "        for term in search_terms:\n",
    "            # Look for exact matches first\n",
    "            exact_matches = [col for col in df.columns if col == term]\n",
    "            if exact_matches:\n",
    "                best_match = exact_matches[0]\n",
    "                break\n",
    "                \n",
    "            # Then look for partial matches\n",
    "            partial_matches = [col for col in df.columns if term.lower() in col.lower()]\n",
    "            if partial_matches:\n",
    "                best_match = partial_matches[0]\n",
    "                break\n",
    "        \n",
    "        if best_match:\n",
    "            index_info['components'].append(best_match)\n",
    "            print(f\"  Found {group_name}: {best_match}\")\n",
    "        else:\n",
    "            print(f\"  No match found for {group_name}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Total components found: {len(index_info['components'])}/{len(index_info['weight_groups'])}\")\n",
    "\n",
    "# Update our composite indices with the improved components and weights\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    if len(index_info['components']) >= 2:  # Require at least 2 components\n",
    "        # Adjust weights based on available components\n",
    "        weights = []\n",
    "        for i, comp in enumerate(index_info['components']):\n",
    "            group_idx = list(index_info['weight_groups'].keys()).index(list(index_info['weight_groups'].keys())[i])\n",
    "            weights.append(index_info['group_weights'][group_idx])\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        # Update the COMPOSITE_INDICES dictionary\n",
    "        COMPOSITE_INDICES[index_name] = {\n",
    "            'components': [f\"{comp}_per90\" for comp in index_info['components']],  # We'll add _per90 suffix later\n",
    "            'weights': weights\n",
    "        }\n",
    "        \n",
    "        print(f\"Updated {index_name} with {len(COMPOSITE_INDICES[index_name]['components'])} components\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f982cc6",
   "metadata": {},
   "source": [
    "## 7. Enhanced per-90 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d454c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying enhanced per-90 normalization...\n",
      "Converting 14 target columns to numeric...\n",
      "Normalizing 14 columns to per-90...\n",
      "Added 14 per-90 normalized columns\n",
      "\n",
      "Sample of per-90 normalized data for index components:\n",
      "               player             team position  minutes  prgp_per90  prgc_per90  succ_per90  kp_per90  sca_per90\n",
      "0     Aaron Cresswell         West Ham       DF    824.0    3.932039    0.655340    0.109223  0.983010   1.638350\n",
      "1      Aaron Ramsdale      Southampton       GK   2700.0    0.000000    0.000000    0.000000  0.000000   0.100000\n",
      "2   Aaron Wan-Bissaka         West Ham       DF   3154.0    4.251744    3.138871    1.826252  0.941661   2.653773\n",
      "3  Abdoulaye Doucouré          Everton       MF   2564.0    2.878315    1.474259    0.842434  0.982839   2.035881\n",
      "4  Abdukodir Khusanov  Manchester City       DF    503.0    4.473161    0.178926    0.000000  0.357853   0.894632\n"
     ]
    }
   ],
   "source": [
    "# Enhanced per-90 normalization that ensures all needed columns are properly converted\n",
    "def enhanced_normalize_per90(df: pd.DataFrame, minutes_col: str = \"minutes\",\n",
    "                           target_columns: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enhanced function to normalize specific columns to per-90 minutes values\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        minutes_col: name of column containing minutes played\n",
    "        target_columns: specific columns to normalize (if None, will normalize all numeric columns)\n",
    "        \n",
    "    returns:\n",
    "        dataframe with additional per-90 normalized columns\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # ensure minutes column exists and is numeric\n",
    "    if minutes_col not in result_df.columns:\n",
    "        raise ValueError(f\"minutes column '{minutes_col}' not found in dataframe\")\n",
    "    \n",
    "    # convert minutes to numeric, handling commas and other formatting\n",
    "    try:\n",
    "        result_df[minutes_col] = pd.to_numeric(\n",
    "            result_df[minutes_col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "            errors='coerce'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to convert minutes column to numeric: {str(e)}\")\n",
    "    \n",
    "    # Check for zero or negative minutes\n",
    "    zero_minutes = (result_df[minutes_col] <= 0).sum()\n",
    "    if zero_minutes > 0:\n",
    "        print(f\"Warning: {zero_minutes} players have zero or negative minutes. Their per-90 values will be NaN.\")\n",
    "    \n",
    "    # If specific target columns are provided, ensure they're all numeric\n",
    "    conversion_failures = []\n",
    "    if target_columns:\n",
    "        print(f\"Converting {len(target_columns)} target columns to numeric...\")\n",
    "        for col in target_columns:\n",
    "            if col in result_df.columns and result_df[col].dtype == 'object':\n",
    "                try:\n",
    "                    result_df[col] = pd.to_numeric(\n",
    "                        result_df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                        errors='coerce'\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    conversion_failures.append(f\"Value error in column '{col}': {str(e)}\")\n",
    "                    print(f\"Warning: Could not convert {col} to numeric: {e}\")\n",
    "                except TypeError as e:\n",
    "                    conversion_failures.append(f\"Type error in column '{col}': {str(e)}\")\n",
    "                    print(f\"Warning: Could not convert {col} to numeric: {e}\")\n",
    "                except Exception as e:\n",
    "                    conversion_failures.append(f\"Unexpected error in column '{col}': {str(e)}\")\n",
    "                    print(f\"Warning: Could not convert {col} to numeric: {e}\")\n",
    "    \n",
    "    # Determine which columns to normalize\n",
    "    cols_to_normalize = []\n",
    "    if target_columns:\n",
    "        # Only normalize specified columns that exist\n",
    "        cols_to_normalize = [col for col in target_columns if col in result_df.columns]\n",
    "    else:\n",
    "        # Normalize all numeric columns except common ID columns and minutes\n",
    "        # Use a default list instead of relying on global variable\n",
    "        default_exclude = ['player', 'team', 'position', 'data_source', 'nation']\n",
    "        exclude_cols = default_exclude + [minutes_col]\n",
    "        \n",
    "        # Also exclude columns that are already per90 or percentages\n",
    "        per90_pattern = r'.*per90.*|.*_90.*|.*pct.*|.*_per_.*|.*ratio.*'\n",
    "        exclude_pattern = result_df.filter(regex=per90_pattern).columns.tolist()\n",
    "        exclude_cols.extend(exclude_pattern)\n",
    "        \n",
    "        # Get all numeric columns that are not in exclude_cols\n",
    "        numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "        cols_to_normalize = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Normalizing {len(cols_to_normalize)} columns to per-90...\")\n",
    "    \n",
    "    # Track normalization issues\n",
    "    normalization_issues = 0\n",
    "    \n",
    "    # normalize each column\n",
    "    for col in cols_to_normalize:\n",
    "        try:\n",
    "            # create new column name\n",
    "            new_col = f\"{col}_per90\"\n",
    "            # normalize to per-90\n",
    "            result_df[new_col] = result_df[col] * 90 / result_df[minutes_col]\n",
    "        except Exception as e:\n",
    "            normalization_issues += 1\n",
    "            print(f\"Warning: Failed to normalize column '{col}': {str(e)}\")\n",
    "    \n",
    "    if normalization_issues > 0:\n",
    "        print(f\"Warning: {normalization_issues} columns had normalization issues\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Get a list of all columns we need for our composite indices\n",
    "columns_for_indices = []\n",
    "for index_info in IMPROVED_INDICES.values():\n",
    "    columns_for_indices.extend(index_info['components'])\n",
    "\n",
    "# Apply the enhanced normalization\n",
    "print(\"Applying enhanced per-90 normalization...\")\n",
    "df_per90 = enhanced_normalize_per90(df, target_columns=columns_for_indices)\n",
    "\n",
    "# Check how many per-90 columns were added\n",
    "original_count = len(df.columns)\n",
    "new_count = len(df_per90.columns)\n",
    "print(f\"Added {new_count - original_count} per-90 normalized columns\")\n",
    "\n",
    "# Display a sample of the original and per-90 columns for our index components\n",
    "sample_cols = ['player', 'team', 'position', 'minutes']\n",
    "per90_cols = [f\"{col}_per90\" for col in columns_for_indices if f\"{col}_per90\" in df_per90.columns][:5]\n",
    "print(\"\\nSample of per-90 normalized data for index components:\")\n",
    "if per90_cols:\n",
    "    print(df_per90[sample_cols + per90_cols].head())\n",
    "else:\n",
    "    print(\"No per-90 columns were created for index components\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126a30c",
   "metadata": {},
   "source": [
    "## 8. Calculate Composite Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1e9062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating composite indices...\n",
      "\n",
      "Calculating PI using 3 components:\n",
      "  - prgp_per90: weight=0.40\n",
      "  - prgc_per90: weight=0.40\n",
      "  - succ_per90: weight=0.20\n",
      "Successfully calculated PI\n",
      "\n",
      "Calculating CCI using 3 components:\n",
      "  - kp_per90: weight=0.30\n",
      "  - sca_per90: weight=0.40\n",
      "  - xag_per90: weight=0.30\n",
      "Successfully calculated CCI\n",
      "\n",
      "Calculating DA using 4 components:\n",
      "  - tkl_per90: weight=0.30\n",
      "  - int_per90: weight=0.30\n",
      "  - recov_per90: weight=0.20\n",
      "  - blocks_per90: weight=0.20\n",
      "Successfully calculated DA\n",
      "\n",
      "Calculating FE using 4 components:\n",
      "  - gls_per90: weight=0.30\n",
      "  - xg_per90: weight=0.30\n",
      "  - sot_per90: weight=0.20\n",
      "  - g_per_sh_per90: weight=0.20\n",
      "Successfully calculated FE\n",
      "\n",
      "4 composite indices calculated\n",
      "\n",
      "Summary statistics for composite indices:\n",
      "                 PI           CCI            DA          FE\n",
      "count  5.620000e+02  5.620000e+02  5.620000e+02  457.000000\n",
      "mean   1.896466e-17  1.580389e-17 -1.580389e-17    0.071869\n",
      "std    7.270364e-01  9.212461e-01  5.258909e-01    0.702697\n",
      "min   -7.049679e-01 -7.561653e-01 -6.855881e-01   -0.337304\n",
      "25%   -3.091966e-01 -5.433816e-01 -2.494794e-01   -0.291096\n",
      "50%   -6.713729e-02 -1.952128e-01 -3.187332e-02   -0.141297\n",
      "75%    2.202494e-01  3.490149e-01  1.789230e-01    0.215905\n",
      "max    1.107167e+01  1.339899e+01  5.351529e+00   10.582667\n"
     ]
    }
   ],
   "source": [
    "# Calculate composite indices using the per-90 normalized columns\n",
    "print(\"\\nCalculating composite indices...\")\n",
    "\n",
    "# Create a function to find per90 versions of our columns\n",
    "def find_per90_columns(df: pd.DataFrame, base_columns: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Find per90 versions of the specified base columns\"\"\"\n",
    "    per90_mapping = {}\n",
    "    for base_col in base_columns:\n",
    "        per90_col = f\"{base_col}_per90\"\n",
    "        if per90_col in df.columns:\n",
    "            per90_mapping[base_col] = per90_col\n",
    "    return per90_mapping\n",
    "\n",
    "# For each index, calculate using the per90 columns\n",
    "indices_calculated = 0\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    if len(index_info['components']) >= 2:  # Require at least 2 components\n",
    "        # Find per90 versions of our components\n",
    "        per90_mapping = find_per90_columns(df_per90, index_info['components'])\n",
    "        \n",
    "        if len(per90_mapping) >= 2:  # Require at least 2 per90 columns\n",
    "            print(f\"\\nCalculating {index_name} using {len(per90_mapping)} components:\")\n",
    "            \n",
    "            # Get the per90 columns and their corresponding weights\n",
    "            per90_columns = list(per90_mapping.values())\n",
    "            \n",
    "            # Get weights for each component\n",
    "            weights = []\n",
    "            for base_col, per90_col in per90_mapping.items():\n",
    "                # Find the component's position in the original components list\n",
    "                idx = index_info['components'].index(base_col)\n",
    "                # Get the corresponding weight group\n",
    "                group_key = list(index_info['weight_groups'].keys())[idx]\n",
    "                # Get the weight for this group\n",
    "                group_idx = list(index_info['weight_groups'].keys()).index(group_key)\n",
    "                weight = index_info['group_weights'][group_idx]\n",
    "                weights.append(weight)\n",
    "                print(f\"  - {per90_col}: weight={weight:.2f}\")\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "            \n",
    "            try:\n",
    "                # Calculate the composite index\n",
    "                df_per90[index_name] = calculate_composite_index(\n",
    "                    df_per90, per90_columns, weights, index_name\n",
    "                )\n",
    "                indices_calculated += 1\n",
    "                print(f\"Successfully calculated {index_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating {index_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Not enough per90 columns found for {index_name}, skipping\")\n",
    "\n",
    "# Display summary statistics for the composite indices\n",
    "print(f\"\\n{indices_calculated} composite indices calculated\")\n",
    "composite_indices_cols = [col for col in df_per90.columns if col in IMPROVED_INDICES.keys()]\n",
    "if composite_indices_cols:\n",
    "    print(\"\\nSummary statistics for composite indices:\")\n",
    "    print(df_per90[composite_indices_cols].describe())\n",
    "else:\n",
    "    print(\"No composite indices were calculated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b63917",
   "metadata": {},
   "source": [
    "## 9. Winsorization at 5th and 95th percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ecb8461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying winsorization at 5.0th and 95.0th percentiles...\n",
      "Winsorized 233 columns\n",
      "\n",
      "Effect of winsorization on selected columns:\n",
      "\n",
      "prgp_per90:\n",
      "  Before - min: 0.00, max: 54.00, mean: 3.34\n",
      "  After  - min: 0.00, max: 7.06, mean: 3.08\n",
      "\n",
      "prgc_per90:\n",
      "  Before - min: 0.00, max: 36.00, mean: 1.79\n",
      "  After  - min: 0.00, max: 5.05, mean: 1.61\n",
      "\n",
      "succ_per90:\n",
      "  Before - min: 0.00, max: 90.00, mean: 0.87\n",
      "  After  - min: 0.00, max: 2.30, mean: 0.66\n",
      "\n",
      "kp_per90:\n",
      "  Before - min: 0.00, max: 24.55, mean: 0.93\n",
      "  After  - min: 0.00, max: 2.44, mean: 0.82\n",
      "\n",
      "sca_per90:\n",
      "  Before - min: 0.00, max: 32.73, mean: 2.13\n",
      "  After  - min: 0.00, max: 5.09, mean: 1.97\n"
     ]
    }
   ],
   "source": [
    "# Apply winsorization to handle outliers\n",
    "print(f\"\\nApplying winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles...\")\n",
    "\n",
    "# Exclude ID columns and composite indices from winsorization\n",
    "exclude_cols = ID_COLUMNS + composite_indices_cols\n",
    "\n",
    "# Winsorize the dataframe\n",
    "df_winsorized = winsorize_df(df_per90, lower=WINSOR_LOWER, upper=WINSOR_UPPER, exclude_cols=exclude_cols)\n",
    "\n",
    "# Check the effect of winsorization on a few columns\n",
    "print(\"\\nEffect of winsorization on selected columns:\")\n",
    "\n",
    "# Select a few numeric columns to check, prioritizing per90 columns\n",
    "numeric_cols = df_per90.select_dtypes(include=[np.number]).columns\n",
    "per90_cols = [col for col in numeric_cols if col.endswith('_per90') and col not in exclude_cols]\n",
    "sample_numeric_cols = per90_cols[:5] if per90_cols else [col for col in numeric_cols if col not in exclude_cols][:5]\n",
    "\n",
    "# Compare before and after winsorization\n",
    "for col in sample_numeric_cols:\n",
    "    before = df_per90[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    after = df_winsorized[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Before - min: {before['min']:.2f}, max: {before['max']:.2f}, mean: {before['mean']:.2f}\")\n",
    "    print(f\"  After  - min: {after['min']:.2f}, max: {after['max']:.2f}, mean: {after['mean']:.2f}\")\n",
    "\n",
    "# Set the winsorized dataframe as our final dataset\n",
    "df_final = df_winsorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11da07e",
   "metadata": {},
   "source": [
    "## 10. Save Dataset & MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc6917",
   "metadata": {},
   "source": [
    "### Output Directory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8c38950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving engineered dataset to ../data/processed/player_stats_engineered.csv...\n",
      "Saved 563 rows and 266 columns\n",
      "\n",
      "Generating metadata...\n",
      "Saving metadata to ../data/processed/feature_metadata.json...\n",
      "Metadata saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Save the engineered dataset\n",
    "print(f\"\\nSaving engineered dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved {df_final.shape[0]} rows and {df_final.shape[1]} columns\")\n",
    "\n",
    "# Generate and save metadata\n",
    "print(f\"\\nGenerating metadata...\")\n",
    "metadata = {\n",
    "    \"num_rows\": len(df_final),\n",
    "    \"num_columns\": len(df_final.columns),\n",
    "    \"feature_groups\": {\n",
    "        \"original\": [col for col in df_final.columns if col in original_columns],\n",
    "        \"per90\": [col for col in df_final.columns if col.endswith('_per90')],\n",
    "        \"composite_indices\": composite_indices_cols\n",
    "    },\n",
    "    \"composite_indices_details\": {}\n",
    "}\n",
    "\n",
    "# Add details about the composite indices\n",
    "for index_name in composite_indices_cols:\n",
    "    if index_name in df_final.columns:\n",
    "        # Find the components used for this index\n",
    "        components = []\n",
    "        weights = []\n",
    "        \n",
    "        # Look through IMPROVED_INDICES to find the components\n",
    "        for base_col, per90_col in find_per90_columns(df_final, IMPROVED_INDICES[index_name]['components']).items():\n",
    "            components.append(per90_col)\n",
    "            \n",
    "            # Find the weight for this component\n",
    "            idx = IMPROVED_INDICES[index_name]['components'].index(base_col)\n",
    "            group_key = list(IMPROVED_INDICES[index_name]['weight_groups'].keys())[idx]\n",
    "            group_idx = list(IMPROVED_INDICES[index_name]['weight_groups'].keys()).index(group_key)\n",
    "            weight = IMPROVED_INDICES[index_name]['group_weights'][group_idx]\n",
    "            weights.append(weight)\n",
    "        \n",
    "        # Normalize weights\n",
    "        if weights:\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        metadata['composite_indices_details'][index_name] = {\n",
    "            'description': {\n",
    "                'PI': \"Progressive Index - measures a player's ball progression abilities\",\n",
    "                'CCI': \"Creative Contribution Index - measures a player's creativity and chance creation\",\n",
    "                'DA': \"Defensive Activity Index - measures a player's defensive contributions\",\n",
    "                'FE': \"Final Execution Index - measures a player's finishing and goal threat\"\n",
    "            }.get(index_name, \"Composite index\"),\n",
    "            'components': components,\n",
    "            'weights': [float(w) for w in weights],\n",
    "            'min_value': float(df_final[index_name].min()),\n",
    "            'max_value': float(df_final[index_name].max()),\n",
    "            'mean_value': float(df_final[index_name].mean()),\n",
    "            'std_value': float(df_final[index_name].std())\n",
    "        }\n",
    "\n",
    "# Save metadata\n",
    "print(f\"Saving metadata to {METADATA_FILE}...\")\n",
    "os.makedirs(os.path.dirname(METADATA_FILE), exist_ok=True)\n",
    "with open(METADATA_FILE, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"Metadata saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9eaf9",
   "metadata": {},
   "source": [
    "### Return Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cf91a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Engineering Summary:\n",
      "- Processed 563 player records\n",
      "- Created 14 per-90 normalized features\n",
      "- Calculated 4 composite indices\n",
      "- Applied winsorization at 5.0th and 95.0th percentiles\n",
      "- Final dataset has 266 columns\n",
      "\n",
      "Next Steps (Phase 3):\n",
      "- Dimensionality Reduction with PCA\n",
      "- Clustering to identify player roles\n",
      "- Interpretation and visualization of clusters\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(f\"- Processed {df_final.shape[0]} player records\")\n",
    "print(f\"- Created {len([col for col in df_final.columns if col.endswith('_per90')])} per-90 normalized features\")\n",
    "print(f\"- Calculated {len(composite_indices_cols)} composite indices\")\n",
    "print(f\"- Applied winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles\")\n",
    "print(f\"- Final dataset has {df_final.shape[1]} columns\")\n",
    "\n",
    "# Print next steps\n",
    "print(\"\\nNext Steps (Phase 3):\")\n",
    "print(\"- Dimensionality Reduction with PCA\")\n",
    "print(\"- Clustering to identify player roles\")\n",
    "print(\"- Interpretation and visualization of clusters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
