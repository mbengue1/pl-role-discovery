{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c4ed8a",
   "metadata": {},
   "source": [
    "# Premier League Player Role Discovery - Phase 2: Feature Engineering\n",
    "\n",
    "This notebook implements the feature engineering phase for the Premier League Player Role Discovery project. It builds on the cleaned dataset from Phase 1 and creates:\n",
    "\n",
    "1. Per-90 normalized versions of relevant statistics\n",
    "2. Composite indices (PI, CCI, DA, FE)\n",
    "3. Winsorized features to handle outliers\n",
    "\n",
    "The final engineered dataset will be saved for use in subsequent clustering and analysis phases.\n",
    "\n",
    "## IMPORTANT: Run Order Instructions\n",
    "\n",
    "To avoid errors, please run the cells in the following order:\n",
    "\n",
    "1. Run cells 1-2: Import Libraries and Define Constants\n",
    "2. Run cells 6-7: Load and Prepare Data (Section 4)\n",
    "3. Run cell 14: Helper Functions (contains all function definitions)\n",
    "4. Run cells 3-5: Data Type Conversion and Column Analysis\n",
    "5. Run cells 8-13: Per-90 Normalization, Composite Indices, and Winsorization\n",
    "6. Run cells 15-16: Save Dataset and Summary\n",
    "\n",
    "This order ensures that all functions are defined before they're used and data is loaded before it's processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969913e8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3de796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Set, Union\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set display options for better dataframe viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c9a14",
   "metadata": {},
   "source": [
    "## 2. Define Constants and Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619272b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "INPUT_FILE = '../data/processed/player_stats_cleaned.csv'\n",
    "OUTPUT_FILE = '../data/processed/player_stats_engineered.csv'\n",
    "METADATA_FILE = '../data/processed/feature_metadata.json'\n",
    "\n",
    "# define constants for feature engineering\n",
    "WINSOR_LOWER = 0.05  # 5th percentile\n",
    "WINSOR_UPPER = 0.95  # 95th percentile\n",
    "\n",
    "# columns that should not be normalized or winsorized\n",
    "ID_COLUMNS = ['player', 'team', 'position', 'data_source', 'nation']\n",
    "\n",
    "# define composite index components and weights\n",
    "COMPOSITE_INDICES = {\n",
    "    'PI': {  # Progressive Index\n",
    "        'components': ['prgp_per90', 'prgc_per90', 'succ_per90'],\n",
    "        'weights': [0.4, 0.4, 0.2]  # progressive passes and carries weighted more\n",
    "    },\n",
    "    'CCI': {  # Creative Contribution Index\n",
    "        'components': ['kp_per90', 'sca_per90', 'xag_per90'],\n",
    "        'weights': [0.3, 0.4, 0.3]  # equal weights with slight emphasis on shot-creating actions\n",
    "    },\n",
    "    'DA': {  # Defensive Activity Index\n",
    "        'components': ['tkl_per90', 'int_per90', 'pressures_per90', 'blocks_per90'],\n",
    "        'weights': [0.3, 0.3, 0.2, 0.2]  # tackles and interceptions weighted more\n",
    "    },\n",
    "    'FE': {  # Final Execution Index\n",
    "        'components': ['gls_per90', 'xg_per90', 'sot_per90', 'g_per_sh'],\n",
    "        'weights': [0.3, 0.3, 0.2, 0.2]  # goals and xG weighted more\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c827b8f",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "### ⚠️ IMPORTANT: Run cell 14 first (contains function definitions) before running other analysis cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and convert string columns to numeric where appropriate\n",
    "print(\"Converting string columns to numeric...\")\n",
    "\n",
    "# Sample a few columns to see their content\n",
    "print(\"\\nSample of column content before conversion:\")\n",
    "for col in ['gls', 'xg', 'prgp', 'tkl', 'minutes']:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].head(3).tolist()}\")\n",
    "\n",
    "# Count numeric columns before conversion\n",
    "numeric_count_before = len(df.select_dtypes(include=[np.number]).columns)\n",
    "print(f\"\\nNumeric columns before conversion: {numeric_count_before}\")\n",
    "\n",
    "# Convert string columns to numeric\n",
    "for col in df.columns:\n",
    "    if col not in ID_COLUMNS and df[col].dtype == 'object':\n",
    "        try:\n",
    "            # Try to convert to numeric, handling commas and other formatting\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                errors='coerce'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# Count numeric columns after conversion\n",
    "numeric_count_after = len(df.select_dtypes(include=[np.number]).columns)\n",
    "print(f\"Numeric columns after conversion: {numeric_count_after}\")\n",
    "print(f\"Converted {numeric_count_after - numeric_count_before} columns to numeric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e61a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Let's examine the column names to better understand what's available\n",
    "print(\"Analyzing column names...\")\n",
    "\n",
    "# Convert all columns to numeric where possible\n",
    "for col in df.columns:\n",
    "    if col not in ID_COLUMNS and df[col].dtype == 'object':\n",
    "        try:\n",
    "            # Try to convert to numeric, handling commas and other formatting\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                errors='coerce'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# Check for key stat columns we need for our indices\n",
    "key_stats = {\n",
    "    'Progressive': ['prgp', 'prgc', 'prog', 'succ', 'carries', 'dribble'],\n",
    "    'Creative': ['kp', 'sca', 'xag', 'xa', 'assist', 'key'],\n",
    "    'Defensive': ['tkl', 'int', 'press', 'block', 'def', 'tack'],\n",
    "    'Finishing': ['gls', 'xg', 'sot', 'shot', 'goal', 'finish']\n",
    "}\n",
    "\n",
    "# Search for columns matching our key stats\n",
    "found_columns = {}\n",
    "for category, terms in key_stats.items():\n",
    "    found_columns[category] = []\n",
    "    for term in terms:\n",
    "        matching_cols = [col for col in df.columns if term.lower() in col.lower()]\n",
    "        if matching_cols:\n",
    "            found_columns[category].extend(matching_cols)\n",
    "\n",
    "# Print the results\n",
    "for category, cols in found_columns.items():\n",
    "    print(f\"\\n{category} columns found ({len(cols)}):\")\n",
    "    for col in sorted(cols)[:10]:  # Show first 10 to avoid too much output\n",
    "        print(f\"  - {col}\")\n",
    "    if len(cols) > 10:\n",
    "        print(f\"  ... and {len(cols) - 10} more\")\n",
    "\n",
    "# Create improved column mappings based on what we found\n",
    "print(\"\\nCreating improved column mappings based on found columns...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved column mappings based on what we found\n",
    "# We'll create per90 versions of these columns in the next step\n",
    "\n",
    "# Define our composite indices with more flexible column options\n",
    "IMPROVED_INDICES = {\n",
    "    'PI': {  # Progressive Index\n",
    "        'components': [],  # Will be populated based on found columns\n",
    "        'weight_groups': {\n",
    "            'passes': ['prgp', 'prgp_passing', 'prgp_possession'],  # 40%\n",
    "            'carries': ['prgc', 'prgc_possession', 'carries'],      # 40%\n",
    "            'dribbles': ['succ', 'succ_possession', 'dribble']      # 20%\n",
    "        },\n",
    "        'group_weights': [0.4, 0.4, 0.2]\n",
    "    },\n",
    "    'CCI': {  # Creative Contribution Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'key_passes': ['kp', 'kp_passing', '1_per_3'],          # 30%\n",
    "            'shot_creation': ['sca', 'sca_creation', 'sca90'],      # 40%\n",
    "            'expected_assists': ['xag', 'xag_passing', 'xa', 'a_xag'] # 30%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.4, 0.3]\n",
    "    },\n",
    "    'DA': {  # Defensive Activity Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'tackles': ['tkl', 'tkl_defense', 'tklw', 'tklw_defense'],  # 30%\n",
    "            'interceptions': ['int', 'int_defense'],                    # 30%\n",
    "            'pressures': ['press', 'pres', 'recov'],                    # 20%\n",
    "            'blocks': ['blocks', 'blocks_defense', 'blocks_pass_types'] # 20%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.3, 0.2, 0.2]\n",
    "    },\n",
    "    'FE': {  # Final Execution Index\n",
    "        'components': [],\n",
    "        'weight_groups': {\n",
    "            'goals': ['gls', 'gls_shooting', 'gls1'],                # 30%\n",
    "            'expected_goals': ['xg', 'xg_shooting', 'xg1', 'npxg'],  # 30%\n",
    "            'shots_on_target': ['sot', 'sot_shooting', 'sh_shooting'], # 20%\n",
    "            'conversion': ['g_per_sh', 'g_per_sot', 'g_xg']          # 20%\n",
    "        },\n",
    "        'group_weights': [0.3, 0.3, 0.2, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# For each index, find the best matching column for each component\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    print(f\"\\nPopulating components for {index_name}:\")\n",
    "    \n",
    "    # For each weight group, find the best matching column\n",
    "    for group_name, search_terms in index_info['weight_groups'].items():\n",
    "        best_match = None\n",
    "        \n",
    "        # Try each search term in order of preference\n",
    "        for term in search_terms:\n",
    "            # Look for exact matches first\n",
    "            exact_matches = [col for col in df.columns if col == term]\n",
    "            if exact_matches:\n",
    "                best_match = exact_matches[0]\n",
    "                break\n",
    "                \n",
    "            # Then look for partial matches\n",
    "            partial_matches = [col for col in df.columns if term.lower() in col.lower()]\n",
    "            if partial_matches:\n",
    "                best_match = partial_matches[0]\n",
    "                break\n",
    "        \n",
    "        if best_match:\n",
    "            index_info['components'].append(best_match)\n",
    "            print(f\"  Found {group_name}: {best_match}\")\n",
    "        else:\n",
    "            print(f\"  No match found for {group_name}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Total components found: {len(index_info['components'])}/{len(index_info['weight_groups'])}\")\n",
    "\n",
    "# Update our composite indices with the improved components and weights\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    if len(index_info['components']) >= 2:  # Require at least 2 components\n",
    "        # Adjust weights based on available components\n",
    "        weights = []\n",
    "        for i, comp in enumerate(index_info['components']):\n",
    "            group_idx = list(index_info['weight_groups'].keys()).index(list(index_info['weight_groups'].keys())[i])\n",
    "            weights.append(index_info['group_weights'][group_idx])\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        # Update the COMPOSITE_INDICES dictionary\n",
    "        COMPOSITE_INDICES[index_name] = {\n",
    "            'components': [f\"{comp}_per90\" for comp in index_info['components']],  # We'll add _per90 suffix later\n",
    "            'weights': weights\n",
    "        }\n",
    "        \n",
    "        print(f\"Updated {index_name} with {len(COMPOSITE_INDICES[index_name]['components'])} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25476401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced per-90 normalization that ensures all needed columns are properly converted\n",
    "def enhanced_normalize_per90(df: pd.DataFrame, minutes_col: str = \"minutes\",\n",
    "                           target_columns: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enhanced function to normalize specific columns to per-90 minutes values\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        minutes_col: name of column containing minutes played\n",
    "        target_columns: specific columns to normalize (if None, will normalize all numeric columns)\n",
    "        \n",
    "    returns:\n",
    "        dataframe with additional per-90 normalized columns\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # ensure minutes column exists and is numeric\n",
    "    if minutes_col not in result_df.columns:\n",
    "        raise ValueError(f\"minutes column '{minutes_col}' not found in dataframe\")\n",
    "    \n",
    "    # convert minutes to numeric, handling commas and other formatting\n",
    "    result_df[minutes_col] = pd.to_numeric(\n",
    "        result_df[minutes_col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # If specific target columns are provided, ensure they're all numeric\n",
    "    if target_columns:\n",
    "        print(f\"Converting {len(target_columns)} target columns to numeric...\")\n",
    "        for col in target_columns:\n",
    "            if col in result_df.columns and result_df[col].dtype == 'object':\n",
    "                try:\n",
    "                    result_df[col] = pd.to_numeric(\n",
    "                        result_df[col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "                        errors='coerce'\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert {col} to numeric: {e}\")\n",
    "    \n",
    "    # Determine which columns to normalize\n",
    "    cols_to_normalize = []\n",
    "    if target_columns:\n",
    "        # Only normalize specified columns that exist\n",
    "        cols_to_normalize = [col for col in target_columns if col in result_df.columns]\n",
    "    else:\n",
    "        # Normalize all numeric columns except ID_COLUMNS and minutes\n",
    "        exclude_cols = ID_COLUMNS + [minutes_col]\n",
    "        # Also exclude columns that are already per90 or percentages\n",
    "        per90_pattern = r'.*per90.*|.*_90.*|.*pct.*|.*_per_.*|.*ratio.*'\n",
    "        exclude_pattern = result_df.filter(regex=per90_pattern).columns.tolist()\n",
    "        exclude_cols.extend(exclude_pattern)\n",
    "        \n",
    "        # Get all numeric columns that are not in exclude_cols\n",
    "        numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "        cols_to_normalize = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Normalizing {len(cols_to_normalize)} columns to per-90...\")\n",
    "    \n",
    "    # normalize each column\n",
    "    for col in cols_to_normalize:\n",
    "        # create new column name\n",
    "        new_col = f\"{col}_per90\"\n",
    "        # normalize to per-90\n",
    "        result_df[new_col] = result_df[col] * 90 / result_df[minutes_col]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Get a list of all columns we need for our composite indices\n",
    "columns_for_indices = []\n",
    "for index_info in IMPROVED_INDICES.values():\n",
    "    columns_for_indices.extend(index_info['components'])\n",
    "\n",
    "# Apply the enhanced normalization\n",
    "print(\"Applying enhanced per-90 normalization...\")\n",
    "df_per90 = enhanced_normalize_per90(df, target_columns=columns_for_indices)\n",
    "\n",
    "# Check how many per-90 columns were added\n",
    "original_count = len(df.columns)\n",
    "new_count = len(df_per90.columns)\n",
    "print(f\"Added {new_count - original_count} per-90 normalized columns\")\n",
    "\n",
    "# Display a sample of the original and per-90 columns for our index components\n",
    "sample_cols = ['player', 'team', 'position', 'minutes']\n",
    "per90_cols = [f\"{col}_per90\" for col in columns_for_indices if f\"{col}_per90\" in df_per90.columns][:5]\n",
    "print(\"\\nSample of per-90 normalized data for index components:\")\n",
    "if per90_cols:\n",
    "    print(df_per90[sample_cols + per90_cols].head())\n",
    "else:\n",
    "    print(\"No per-90 columns were created for index components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate composite indices using the per-90 normalized columns\n",
    "print(\"\\nCalculating composite indices...\")\n",
    "\n",
    "# Create a function to find per90 versions of our columns\n",
    "def find_per90_columns(df: pd.DataFrame, base_columns: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Find per90 versions of the specified base columns\"\"\"\n",
    "    per90_mapping = {}\n",
    "    for base_col in base_columns:\n",
    "        per90_col = f\"{base_col}_per90\"\n",
    "        if per90_col in df.columns:\n",
    "            per90_mapping[base_col] = per90_col\n",
    "    return per90_mapping\n",
    "\n",
    "# For each index, calculate using the per90 columns\n",
    "indices_calculated = 0\n",
    "for index_name, index_info in IMPROVED_INDICES.items():\n",
    "    if len(index_info['components']) >= 2:  # Require at least 2 components\n",
    "        # Find per90 versions of our components\n",
    "        per90_mapping = find_per90_columns(df_per90, index_info['components'])\n",
    "        \n",
    "        if len(per90_mapping) >= 2:  # Require at least 2 per90 columns\n",
    "            print(f\"\\nCalculating {index_name} using {len(per90_mapping)} components:\")\n",
    "            \n",
    "            # Get the per90 columns and their corresponding weights\n",
    "            per90_columns = list(per90_mapping.values())\n",
    "            \n",
    "            # Get weights for each component\n",
    "            weights = []\n",
    "            for base_col, per90_col in per90_mapping.items():\n",
    "                # Find the component's position in the original components list\n",
    "                idx = index_info['components'].index(base_col)\n",
    "                # Get the corresponding weight group\n",
    "                group_key = list(index_info['weight_groups'].keys())[idx]\n",
    "                # Get the weight for this group\n",
    "                group_idx = list(index_info['weight_groups'].keys()).index(group_key)\n",
    "                weight = index_info['group_weights'][group_idx]\n",
    "                weights.append(weight)\n",
    "                print(f\"  - {per90_col}: weight={weight:.2f}\")\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "            \n",
    "            try:\n",
    "                # Calculate the composite index\n",
    "                df_per90[index_name] = calculate_composite_index(\n",
    "                    df_per90, per90_columns, weights, index_name\n",
    "                )\n",
    "                indices_calculated += 1\n",
    "                print(f\"Successfully calculated {index_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating {index_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Not enough per90 columns found for {index_name}, skipping\")\n",
    "\n",
    "# Display summary statistics for the composite indices\n",
    "print(f\"\\n{indices_calculated} composite indices calculated\")\n",
    "composite_indices_cols = [col for col in df_per90.columns if col in IMPROVED_INDICES.keys()]\n",
    "if composite_indices_cols:\n",
    "    print(\"\\nSummary statistics for composite indices:\")\n",
    "    print(df_per90[composite_indices_cols].describe())\n",
    "else:\n",
    "    print(\"No composite indices were calculated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43fcbf",
   "metadata": {},
   "source": [
    "# ⚠️ IMPORTANT: Run cell 14 first! ⚠️\n",
    "# This cell requires the calculate_composite_index function to be defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178660b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply winsorization to handle outliers\n",
    "print(f\"\\nApplying winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles...\")\n",
    "\n",
    "# Exclude ID columns and composite indices from winsorization\n",
    "exclude_cols = ID_COLUMNS + composite_indices_cols\n",
    "\n",
    "# Winsorize the dataframe\n",
    "df_winsorized = winsorize_df(df_per90, lower=WINSOR_LOWER, upper=WINSOR_UPPER, exclude_cols=exclude_cols)\n",
    "\n",
    "# Check the effect of winsorization on a few columns\n",
    "print(\"\\nEffect of winsorization on selected columns:\")\n",
    "\n",
    "# Select a few numeric columns to check, prioritizing per90 columns\n",
    "numeric_cols = df_per90.select_dtypes(include=[np.number]).columns\n",
    "per90_cols = [col for col in numeric_cols if col.endswith('_per90') and col not in exclude_cols]\n",
    "sample_numeric_cols = per90_cols[:5] if per90_cols else [col for col in numeric_cols if col not in exclude_cols][:5]\n",
    "\n",
    "# Compare before and after winsorization\n",
    "for col in sample_numeric_cols:\n",
    "    before = df_per90[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    after = df_winsorized[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Before - min: {before['min']:.2f}, max: {before['max']:.2f}, mean: {before['mean']:.2f}\")\n",
    "    print(f\"  After  - min: {after['min']:.2f}, max: {after['max']:.2f}, mean: {after['mean']:.2f}\")\n",
    "\n",
    "# Set the winsorized dataframe as our final dataset\n",
    "df_final = df_winsorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6317f55",
   "metadata": {},
   "source": [
    "# ⚠️ IMPORTANT EXECUTION ORDER ⚠️\n",
    "\n",
    "To avoid errors like \"name 'winsorize_df' is not defined\" or \"name 'calculate_composite_index' is not defined\", please run cell 14 (the large cell with all helper function definitions) BEFORE running this cell.\n",
    "\n",
    "The correct execution order is:\n",
    "1. Run cells 1-2: Import Libraries and Define Constants\n",
    "2. Run cells 6-7: Load and Prepare Data (Section 4) \n",
    "3. Run cell 14: Helper Functions (contains all function definitions)\n",
    "4. Then continue with the analysis cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c38950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Save the engineered dataset\n",
    "print(f\"\\nSaving engineered dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved {df_final.shape[0]} rows and {df_final.shape[1]} columns\")\n",
    "\n",
    "# Generate and save metadata\n",
    "print(f\"\\nGenerating metadata...\")\n",
    "metadata = {\n",
    "    \"num_rows\": len(df_final),\n",
    "    \"num_columns\": len(df_final.columns),\n",
    "    \"feature_groups\": {\n",
    "        \"original\": [col for col in df_final.columns if col in original_columns],\n",
    "        \"per90\": [col for col in df_final.columns if col.endswith('_per90')],\n",
    "        \"composite_indices\": composite_indices_cols\n",
    "    },\n",
    "    \"composite_indices_details\": {}\n",
    "}\n",
    "\n",
    "# Add details about the composite indices\n",
    "for index_name in composite_indices_cols:\n",
    "    if index_name in df_final.columns:\n",
    "        # Find the components used for this index\n",
    "        components = []\n",
    "        weights = []\n",
    "        \n",
    "        # Look through IMPROVED_INDICES to find the components\n",
    "        for base_col, per90_col in find_per90_columns(df_final, IMPROVED_INDICES[index_name]['components']).items():\n",
    "            components.append(per90_col)\n",
    "            \n",
    "            # Find the weight for this component\n",
    "            idx = IMPROVED_INDICES[index_name]['components'].index(base_col)\n",
    "            group_key = list(IMPROVED_INDICES[index_name]['weight_groups'].keys())[idx]\n",
    "            group_idx = list(IMPROVED_INDICES[index_name]['weight_groups'].keys()).index(group_key)\n",
    "            weight = IMPROVED_INDICES[index_name]['group_weights'][group_idx]\n",
    "            weights.append(weight)\n",
    "        \n",
    "        # Normalize weights\n",
    "        if weights:\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        metadata['composite_indices_details'][index_name] = {\n",
    "            'description': {\n",
    "                'PI': \"Progressive Index - measures a player's ball progression abilities\",\n",
    "                'CCI': \"Creative Contribution Index - measures a player's creativity and chance creation\",\n",
    "                'DA': \"Defensive Activity Index - measures a player's defensive contributions\",\n",
    "                'FE': \"Final Execution Index - measures a player's finishing and goal threat\"\n",
    "            }.get(index_name, \"Composite index\"),\n",
    "            'components': components,\n",
    "            'weights': [float(w) for w in weights],\n",
    "            'min_value': float(df_final[index_name].min()),\n",
    "            'max_value': float(df_final[index_name].max()),\n",
    "            'mean_value': float(df_final[index_name].mean()),\n",
    "            'std_value': float(df_final[index_name].std())\n",
    "        }\n",
    "\n",
    "# Save metadata\n",
    "print(f\"Saving metadata to {METADATA_FILE}...\")\n",
    "os.makedirs(os.path.dirname(METADATA_FILE), exist_ok=True)\n",
    "with open(METADATA_FILE, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"Metadata saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf91a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(f\"- Processed {df_final.shape[0]} player records\")\n",
    "print(f\"- Created {len([col for col in df_final.columns if col.endswith('_per90')])} per-90 normalized features\")\n",
    "print(f\"- Calculated {len(composite_indices_cols)} composite indices\")\n",
    "print(f\"- Applied winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles\")\n",
    "print(f\"- Final dataset has {df_final.shape[1]} columns\")\n",
    "\n",
    "# Print next steps\n",
    "print(\"\\nNext Steps (Phase 3):\")\n",
    "print(\"- Dimensionality Reduction with PCA\")\n",
    "print(\"- Clustering to identify player roles\")\n",
    "print(\"- Interpretation and visualization of clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per90(df: pd.DataFrame, minutes_col: str = \"minutes\", \n",
    "                  exclude_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    normalize raw counting stats to per-90 minutes values\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        minutes_col: name of column containing minutes played\n",
    "        exclude_cols: list of columns to exclude from normalization\n",
    "        \n",
    "    returns:\n",
    "        dataframe with additional per-90 normalized columns\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # ensure minutes column exists and is numeric\n",
    "    if minutes_col not in result_df.columns:\n",
    "        raise ValueError(f\"minutes column '{minutes_col}' not found in dataframe\")\n",
    "    \n",
    "    # convert minutes to numeric, handling commas and other formatting\n",
    "    result_df[minutes_col] = pd.to_numeric(\n",
    "        result_df[minutes_col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # default exclude columns if none provided\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ID_COLUMNS.copy()\n",
    "    else:\n",
    "        exclude_cols = exclude_cols.copy()\n",
    "        \n",
    "    # add minutes column to exclude list if not already there\n",
    "    if minutes_col not in exclude_cols:\n",
    "        exclude_cols.append(minutes_col)\n",
    "    \n",
    "    # identify columns that are already per90 or percentages\n",
    "    per90_pattern = r'.*per90.*|.*_90.*|.*pct.*|.*_per_.*|.*ratio.*'\n",
    "    exclude_pattern = result_df.filter(regex=per90_pattern).columns.tolist()\n",
    "    exclude_cols.extend(exclude_pattern)\n",
    "    \n",
    "    # get numeric columns that are not in exclude_cols\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    cols_to_normalize = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # normalize each column\n",
    "    for col in cols_to_normalize:\n",
    "        # create new column name\n",
    "        new_col = f\"{col}_per90\"\n",
    "        # normalize to per-90\n",
    "        result_df[new_col] = result_df[col] * 90 / result_df[minutes_col]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def calculate_composite_index(df: pd.DataFrame, features: List[str], \n",
    "                             weights: List[float], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    calculate a composite index from multiple features using weights\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        features: list of feature names to include in the index\n",
    "        weights: list of weights for each feature (must sum to 1)\n",
    "        name: name of the composite index\n",
    "        \n",
    "    returns:\n",
    "        series with the calculated composite index\n",
    "    \"\"\"\n",
    "    # validate inputs\n",
    "    if len(features) != len(weights):\n",
    "        raise ValueError(\"features and weights must have the same length\")\n",
    "    \n",
    "    if abs(sum(weights) - 1.0) > 0.001:\n",
    "        raise ValueError(\"weights must sum to 1.0\")\n",
    "    \n",
    "    # check if all features exist in the dataframe\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"features {missing_features} not found in dataframe\")\n",
    "    \n",
    "    # create a copy of the features for standardization\n",
    "    features_df = df[features].copy()\n",
    "    \n",
    "    # standardize each feature\n",
    "    scaler = StandardScaler()\n",
    "    features_standardized = scaler.fit_transform(features_df)\n",
    "    \n",
    "    # convert back to dataframe for easier handling\n",
    "    features_std_df = pd.DataFrame(features_standardized, columns=features, index=df.index)\n",
    "    \n",
    "    # calculate weighted sum\n",
    "    composite_index = pd.Series(0, index=df.index)\n",
    "    for feature, weight in zip(features, weights):\n",
    "        composite_index += features_std_df[feature] * weight\n",
    "    \n",
    "    return composite_index\n",
    "\n",
    "def winsorize_df(df: pd.DataFrame, lower: float = 0.05, upper: float = 0.95,\n",
    "                exclude_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    apply winsorization to handle outliers in numeric columns\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        lower: lower percentile for winsorization (default: 0.05)\n",
    "        upper: upper percentile for winsorization (default: 0.95)\n",
    "        exclude_cols: list of columns to exclude from winsorization\n",
    "        \n",
    "    returns:\n",
    "        dataframe with winsorized values\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # default exclude columns if none provided\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ID_COLUMNS.copy()\n",
    "    \n",
    "    # get numeric columns that are not in exclude_cols\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    cols_to_winsorize = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # winsorize each column\n",
    "    for col in cols_to_winsorize:\n",
    "        # skip columns with all NaN values\n",
    "        if result_df[col].isna().all():\n",
    "            continue\n",
    "            \n",
    "        # winsorize the column\n",
    "        result_df[col] = stats.mstats.winsorize(result_df[col], limits=[lower, 1-upper])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def generate_feature_metadata(df: pd.DataFrame, original_cols: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    generate metadata about features in the dataframe\n",
    "    \n",
    "    args:\n",
    "        df: dataframe with player statistics\n",
    "        original_cols: list of column names from the original dataframe\n",
    "        \n",
    "    returns:\n",
    "        dictionary with feature metadata\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"num_rows\": len(df),\n",
    "        \"num_columns\": len(df.columns),\n",
    "        \"feature_groups\": {\n",
    "            \"original\": [col for col in df.columns if col in original_cols],\n",
    "            \"per90\": [col for col in df.columns if col.endswith('_per90')],\n",
    "            \"composite_indices\": [\n",
    "                \"PI\", \"CCI\", \"DA\", \"FE\"\n",
    "            ]\n",
    "        },\n",
    "        \"composite_indices\": {\n",
    "            index_name: {\n",
    "                \"description\": {\n",
    "                    \"PI\": \"Progressive Index - measures a player's ball progression abilities\",\n",
    "                    \"CCI\": \"Creative Contribution Index - measures a player's creativity and chance creation\",\n",
    "                    \"DA\": \"Defensive Activity Index - measures a player's defensive contributions\",\n",
    "                    \"FE\": \"Final Execution Index - measures a player's finishing and goal threat\"\n",
    "                }[index_name],\n",
    "                \"components\": components[\"components\"],\n",
    "                \"weights\": components[\"weights\"]\n",
    "            }\n",
    "            for index_name, components in COMPOSITE_INDICES.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bbd52",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67478608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned dataset\n",
    "print(f\"loading data from {INPUT_FILE}\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"error loading data: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"found {duplicate_count} duplicate rows\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"removing duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"dataset now has {df.shape[0]} rows\")\n",
    "\n",
    "# check for missing values in key fields\n",
    "key_fields = ['player', 'team', 'position', 'minutes']\n",
    "missing_key_fields = df[key_fields].isna().sum()\n",
    "print(\"\\nmissing values in key fields:\")\n",
    "print(missing_key_fields)\n",
    "\n",
    "if missing_key_fields.sum() > 0:\n",
    "    print(\"removing rows with missing key fields...\")\n",
    "    df = df.dropna(subset=key_fields)\n",
    "    print(f\"dataset now has {df.shape[0]} rows\")\n",
    "\n",
    "# store original column names for metadata\n",
    "original_columns = df.columns.tolist()\n",
    "\n",
    "# display basic info about the dataset\n",
    "print(\"\\ndataset info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a198808",
   "metadata": {},
   "source": [
    "## 5. Per-90 Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize raw counting stats to per-90 minutes\n",
    "print(\"applying per-90 normalization...\")\n",
    "df_per90 = normalize_per90(df, minutes_col=\"minutes\")\n",
    "\n",
    "# check how many per-90 columns were added\n",
    "original_count = len(df.columns)\n",
    "new_count = len(df_per90.columns)\n",
    "print(f\"added {new_count - original_count} per-90 normalized columns\")\n",
    "\n",
    "# display a sample of the original and per-90 columns\n",
    "sample_cols = ['player', 'team', 'position', 'minutes']\n",
    "per90_cols = [col for col in df_per90.columns if col.endswith('_per90')][:5]  # first 5 per90 cols\n",
    "print(\"\\nsample of per-90 normalized data:\")\n",
    "print(df_per90[sample_cols + per90_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13652a01",
   "metadata": {},
   "source": [
    "## 6. Calculate Composite Indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a mapping for columns that might have different names in our dataset\n",
    "column_mapping = {\n",
    "    # progressive index\n",
    "    'prgp_per90': ['prgp_per90', 'prgp_passing_per90'],  # progressive passes\n",
    "    'prgc_per90': ['prgc_per90', 'prgc_possession_per90'],  # progressive carries\n",
    "    'succ_per90': ['succ_per90', 'succ_possession_per90'],  # successful dribbles\n",
    "    \n",
    "    # creative contribution index\n",
    "    'kp_per90': ['kp_per90', 'kp_passing_per90'],  # key passes\n",
    "    'sca_per90': ['sca_per90', 'sca_creation_per90'],  # shot-creating actions\n",
    "    'xag_per90': ['xag_per90', 'xag_passing_per90', 'xag1_per90'],  # expected assists\n",
    "    \n",
    "    # defensive activity index\n",
    "    'tkl_per90': ['tkl_per90', 'tkl_defense_per90'],  # tackles\n",
    "    'int_per90': ['int_per90', 'int_defense_per90'],  # interceptions\n",
    "    'pressures_per90': ['pressures_per90', 'press_per90', 'pres_per90'],  # pressures\n",
    "    'blocks_per90': ['blocks_per90', 'blocks_defense_per90'],  # blocks\n",
    "    \n",
    "    # final execution index\n",
    "    'gls_per90': ['gls_per90', 'gls_shooting_per90', 'gls1_per90'],  # goals\n",
    "    'xg_per90': ['xg_per90', 'xg_shooting_per90', 'xg1_per90'],  # expected goals\n",
    "    'sot_per90': ['sot_per90', 'sot_shooting_per90'],  # shots on target\n",
    "    'g_per_sh': ['g_per_sh', 'g_per_sh_shooting']  # goal per shot ratio\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76901847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the actual column name in the dataframe\n",
    "def find_column(df: pd.DataFrame, possible_names: List[str]) -> Optional[str]:\n",
    "    \"\"\"find the first column name from a list that exists in the dataframe\"\"\"\n",
    "    for col in possible_names:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# check which components are available in our dataset\n",
    "print(\"checking for composite index components...\")\n",
    "available_components = {}\n",
    "\n",
    "for index_name, index_info in COMPOSITE_INDICES.items():\n",
    "    available_components[index_name] = []\n",
    "    \n",
    "    for component in index_info['components']:\n",
    "        # get possible column names for this component\n",
    "        possible_names = column_mapping.get(component, [component])\n",
    "        \n",
    "        # find the actual column name in the dataframe\n",
    "        actual_col = find_column(df_per90, possible_names)\n",
    "        \n",
    "        if actual_col:\n",
    "            available_components[index_name].append(actual_col)\n",
    "            print(f\"found {component} as {actual_col}\")\n",
    "        else:\n",
    "            print(f\"warning: component {component} not found in dataset\")\n",
    "\n",
    "# update composite indices with available components\n",
    "updated_indices = {}\n",
    "for index_name, components in available_components.items():\n",
    "    if len(components) >= 2:  # require at least 2 components\n",
    "        # get the original weights\n",
    "        original_components = COMPOSITE_INDICES[index_name]['components']\n",
    "        original_weights = COMPOSITE_INDICES[index_name]['weights']\n",
    "        \n",
    "        # create a mapping from original components to weights\n",
    "        weight_map = {comp: weight for comp, weight in zip(original_components, original_weights)}\n",
    "        \n",
    "        # get weights for available components\n",
    "        weights = [weight_map.get(comp.replace('_possession', '').replace('_passing', '').replace('_defense', '').replace('_shooting', '').replace('_creation', '').replace('1', ''), 1.0) \n",
    "                  for comp in components]\n",
    "        \n",
    "        # normalize weights to sum to 1\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "        updated_indices[index_name] = {\n",
    "            'components': components,\n",
    "            'weights': weights\n",
    "        }\n",
    "    else:\n",
    "        print(f\"warning: not enough components found for {index_name}, skipping\")\n",
    "\n",
    "print(\"\\nupdated composite indices:\")\n",
    "for index_name, index_info in updated_indices.items():\n",
    "    print(f\"{index_name}: {len(index_info['components'])} components\")\n",
    "    for comp, weight in zip(index_info['components'], index_info['weights']):\n",
    "        print(f\"  - {comp}: {weight:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate composite indices\n",
    "print(\"\\ncalculating composite indices...\")\n",
    "for index_name, index_info in updated_indices.items():\n",
    "    components = index_info['components']\n",
    "    weights = index_info['weights']\n",
    "    \n",
    "    try:\n",
    "        # calculate the composite index\n",
    "        df_per90[index_name] = calculate_composite_index(\n",
    "            df_per90, components, weights, index_name\n",
    "        )\n",
    "        print(f\"calculated {index_name} using {len(components)} components\")\n",
    "    except Exception as e:\n",
    "        print(f\"error calculating {index_name}: {e}\")\n",
    "\n",
    "# display summary statistics for the composite indices\n",
    "print(\"\\nsummary statistics for composite indices:\")\n",
    "composite_indices_cols = [col for col in df_per90.columns if col in updated_indices.keys()]\n",
    "if composite_indices_cols:\n",
    "    print(df_per90[composite_indices_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f6677",
   "metadata": {},
   "source": [
    "## 7. Apply Winsorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply winsorization to handle outliers\n",
    "print(f\"applying winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles...\")\n",
    "\n",
    "# exclude ID columns and composite indices from winsorization\n",
    "exclude_cols = ID_COLUMNS + composite_indices_cols\n",
    "\n",
    "# winsorize the dataframe\n",
    "df_winsorized = winsorize_df(df_per90, lower=WINSOR_LOWER, upper=WINSOR_UPPER, exclude_cols=exclude_cols)\n",
    "\n",
    "# check the effect of winsorization on a few columns\n",
    "print(\"\\neffect of winsorization on selected columns:\")\n",
    "\n",
    "# select a few numeric columns to check\n",
    "numeric_cols = df_per90.select_dtypes(include=[np.number]).columns\n",
    "sample_numeric_cols = [col for col in numeric_cols if col not in exclude_cols][:5]  # first 5 numeric cols\n",
    "\n",
    "# compare before and after winsorization\n",
    "for col in sample_numeric_cols:\n",
    "    before = df_per90[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    after = df_winsorized[col].describe([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Before - min: {before['min']:.2f}, max: {before['max']:.2f}, mean: {before['mean']:.2f}\")\n",
    "    print(f\"  After  - min: {after['min']:.2f}, max: {after['max']:.2f}, mean: {after['mean']:.2f}\")\n",
    "\n",
    "# set the winsorized dataframe as our final dataset\n",
    "df_final = df_winsorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de6129",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# save the engineered dataset\n",
    "print(f\"saving engineered dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"saved {df_final.shape[0]} rows and {df_final.shape[1]} columns\")\n",
    "\n",
    "# generate and save metadata\n",
    "print(f\"\\ngenerating metadata...\")\n",
    "metadata = generate_feature_metadata(df_final, original_columns)\n",
    "\n",
    "# add additional metadata about the composite indices\n",
    "metadata['composite_indices_details'] = {}\n",
    "for index_name, index_info in updated_indices.items():\n",
    "    if index_name in df_final.columns:\n",
    "        metadata['composite_indices_details'][index_name] = {\n",
    "            'components': index_info['components'],\n",
    "            'weights': [float(w) for w in index_info['weights']],\n",
    "            'min_value': float(df_final[index_name].min()),\n",
    "            'max_value': float(df_final[index_name].max()),\n",
    "            'mean_value': float(df_final[index_name].mean()),\n",
    "            'std_value': float(df_final[index_name].std())\n",
    "        }\n",
    "\n",
    "# save metadata\n",
    "print(f\"saving metadata to {METADATA_FILE}...\")\n",
    "os.makedirs(os.path.dirname(METADATA_FILE), exist_ok=True)\n",
    "with open(METADATA_FILE, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"metadata saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f41c3e",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef056b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary statistics\n",
    "print(\"Feature Engineering Summary:\")\n",
    "print(f\"- Processed {df_final.shape[0]} player records\")\n",
    "print(f\"- Created {len([col for col in df_final.columns if col.endswith('_per90')])} per-90 normalized features\")\n",
    "print(f\"- Calculated {len([col for col in df_final.columns if col in updated_indices.keys()])} composite indices\")\n",
    "print(f\"- Applied winsorization at {WINSOR_LOWER*100}th and {WINSOR_UPPER*100}th percentiles\")\n",
    "print(f\"- Final dataset has {df_final.shape[1]} columns\")\n",
    "\n",
    "# print next steps\n",
    "print(\"\\nNext Steps (Phase 3):\")\n",
    "print(\"- Dimensionality Reduction with PCA\")\n",
    "print(\"- Clustering to identify player roles\")\n",
    "print(\"- Interpretation and visualization of clusters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
