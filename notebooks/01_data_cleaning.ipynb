{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dea7ee",
   "metadata": {},
   "source": [
    "# Premier League Player Role Discovery - Phase 1: Data Cleaning\n",
    "\n",
    "This notebook handles the data cleaning process for the Premier League Player Role Discovery project. It loads player statistics from multiple CSV files, harmonizes column names, aggregates player data across teams, filters for outfield players with sufficient minutes, and saves the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc8610",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "71b1feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import warnings\n",
    "\n",
    "# suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set display options for better dataframe viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ebc21",
   "metadata": {},
   "source": [
    "## 2. Define Constants and Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3b61900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "RAW_DATA_DIR = '../data/raw/fbref_2024_25/'\n",
    "PROCESSED_DATA_DIR = '../data/processed/'\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DATA_DIR, 'player_stats_cleaned.csv')\n",
    "SCHEMA_FILE = os.path.join(PROCESSED_DATA_DIR, 'data_schema.json')\n",
    "\n",
    "# minimum minutes threshold (as per requirements)\n",
    "MIN_MINUTES = 600\n",
    "\n",
    "# list of positions to exclude (goalkeepers)\n",
    "EXCLUDED_POSITIONS = ['GK']\n",
    "\n",
    "# key columns that must be present in the final dataset\n",
    "REQUIRED_COLUMNS = ['player', 'team', 'position', 'minutes']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45634846",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "63515044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_csv_files(directory: str) -> List[str]:\n",
    "    \"\"\"list all csv files in the given directory\"\"\"\n",
    "    return glob.glob(os.path.join(directory, '*.csv'))\n",
    "\n",
    "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"standardize column names to lowercase with underscores\"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # clean column names\n",
    "    df.columns = [\n",
    "        col.lower()\n",
    "           .replace(' ', '_')\n",
    "           .replace('/', '_per_')\n",
    "           .replace('%', 'pct')\n",
    "           .replace('+', 'plus')\n",
    "           .replace('-', '_')\n",
    "           .replace('(', '')\n",
    "           .replace(')', '')\n",
    "           .replace('.', '')\n",
    "           .replace('__', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def identify_player_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"identify the column containing player names\"\"\"\n",
    "    # For FBref data, the column is always 'Player'\n",
    "    if 'Player' in df.columns:\n",
    "        return 'Player'\n",
    "    \n",
    "    # Fallbacks for other formats\n",
    "    player_column_candidates = ['player', 'player_name', 'name']\n",
    "    \n",
    "    # Check exact matches\n",
    "    for col in player_column_candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    \n",
    "    # Case insensitive search\n",
    "    for col in df.columns:\n",
    "        if any(c.lower() == col.lower() for c in player_column_candidates):\n",
    "            return col\n",
    "    \n",
    "    # If we have a row_id column (added during loading), use that as a last resort\n",
    "    if 'row_id' in df.columns:\n",
    "        return 'row_id'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def identify_minutes_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"identify the column containing minutes played\"\"\"\n",
    "    # For FBref data, the column is always 'Min'\n",
    "    if 'Min' in df.columns:\n",
    "        return 'Min'\n",
    "    \n",
    "    # Fallbacks for other formats\n",
    "    minutes_column_candidates = ['minutes', 'min', 'minutes_played', 'mp', 'MP']\n",
    "    \n",
    "    # Check exact matches\n",
    "    for col in minutes_column_candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    \n",
    "    # Case insensitive search\n",
    "    for col in df.columns:\n",
    "        if any(c.lower() == col.lower() for c in minutes_column_candidates):\n",
    "            return col\n",
    "            \n",
    "    return None\n",
    "\n",
    "def identify_position_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"identify the column containing player positions\"\"\"\n",
    "    # For FBref data, the column is always 'Pos'\n",
    "    if 'Pos' in df.columns:\n",
    "        return 'Pos'\n",
    "    \n",
    "    # Fallbacks for other formats\n",
    "    position_column_candidates = ['position', 'pos', 'player_position']\n",
    "    \n",
    "    # Check exact matches\n",
    "    for col in position_column_candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    \n",
    "    # Case insensitive search\n",
    "    for col in df.columns:\n",
    "        if any(c.lower() == col.lower() for c in position_column_candidates):\n",
    "            return col\n",
    "            \n",
    "    return None\n",
    "\n",
    "def identify_team_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"identify the column containing team names\"\"\"\n",
    "    # For FBref data, the column is always 'Squad'\n",
    "    if 'Squad' in df.columns:\n",
    "        return 'Squad'\n",
    "    \n",
    "    # Fallbacks for other formats\n",
    "    team_column_candidates = ['squad', 'team', 'club']\n",
    "    \n",
    "    # Check exact matches\n",
    "    for col in team_column_candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    \n",
    "    # Case insensitive search\n",
    "    for col in df.columns:\n",
    "        if any(c.lower() == col.lower() for c in team_column_candidates):\n",
    "            return col\n",
    "            \n",
    "    return None\n",
    "\n",
    "def standardize_column_names(df: pd.DataFrame, column_mapping: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"rename columns according to the provided mapping\"\"\"\n",
    "    # create a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # rename only the columns that exist in the dataframe\n",
    "    existing_cols = {old: new for old, new in column_mapping.items() if old in df.columns}\n",
    "    df = df.rename(columns=existing_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_outfield_players(df: pd.DataFrame, position_col: str, excluded_positions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"filter out goalkeepers and keep only outfield players\"\"\"\n",
    "    if position_col in df.columns:\n",
    "        return df[~df[position_col].isin(excluded_positions)]\n",
    "    return df\n",
    "\n",
    "def filter_by_minutes(df: pd.DataFrame, minutes_col: str, min_minutes: int) -> pd.DataFrame:\n",
    "    \"\"\"filter players with at least the minimum required minutes\"\"\"\n",
    "    if minutes_col in df.columns:\n",
    "        # Create a copy to avoid modifying the original dataframe\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert minutes to numeric, handling commas and other formatting\n",
    "        df[minutes_col] = pd.to_numeric(\n",
    "            df[minutes_col].astype(str).str.replace(',', '').str.replace('-', '0'),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Filter by minimum minutes\n",
    "        return df[df[minutes_col] >= min_minutes]\n",
    "    return df\n",
    "\n",
    "def validate_schema(df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
    "    \"\"\"validate that the dataframe contains all required columns\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"error: missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec9dfb",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28372f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 9 csv files in ../data/raw/fbref_2024_25/\n",
      "- fbref_misc_2024_25.csv\n",
      "- fbref_passing_2024_25.csv\n",
      "- fbref_playtime_2024_25.csv\n",
      "- fbref_defense_2024_25.csv\n",
      "- fbref_standard_2024_25.csv\n",
      "- fbref_pass_types_2024_25.csv\n",
      "- fbref_creation_2024_25.csv\n",
      "- fbref_possession_2024_25.csv\n",
      "- fbref_shooting_2024_25.csv\n"
     ]
    }
   ],
   "source": [
    "# list all csv files in the raw data directory\n",
    "csv_files = list_csv_files(RAW_DATA_DIR)\n",
    "print(f\"found {len(csv_files)} csv files in {RAW_DATA_DIR}\")\n",
    "\n",
    "# display the file names\n",
    "for file in csv_files:\n",
    "    print(f\"- {os.path.basename(file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1d90d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data looks good: found 596 players with names\n",
      "loaded misc data: 596 rows, 25 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded passing data: 596 rows, 32 columns\n",
      "Data looks good: found 730 players with names\n",
      "loaded playtime data: 730 rows, 30 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded defense data: 596 rows, 25 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded standard data: 596 rows, 37 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded pass_types data: 596 rows, 24 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded creation data: 596 rows, 25 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded possession data: 596 rows, 31 columns\n",
      "Data looks good: found 596 players with names\n",
      "loaded shooting data: 596 rows, 26 columns\n"
     ]
    }
   ],
   "source": [
    "# load each csv file into a dictionary of dataframes\n",
    "raw_data = {}\n",
    "for file in csv_files:\n",
    "    table_name = os.path.basename(file).replace('fbref_', '').replace('_2024_25.csv', '')\n",
    "    try:\n",
    "        # FBref CSV files have a specific format:\n",
    "        # - First row contains category headers\n",
    "        # - Second row contains the actual column names\n",
    "        # We need to skip the first row and use the second row as headers\n",
    "        \n",
    "        # Read the file with the second row as headers\n",
    "        df = pd.read_csv(file, header=1)\n",
    "        \n",
    "        # Clean up the 'Matches' column which often appears at the end of FBref tables\n",
    "        if 'Matches' in df.columns:\n",
    "            df = df.drop('Matches', axis=1)\n",
    "        \n",
    "        # Check if we have valid data - the first row should have Rk=1\n",
    "        if 'Rk' in df.columns and len(df) > 0:\n",
    "            # Ensure we have the player data\n",
    "            if 'Player' in df.columns and df['Player'].notna().sum() > 0:\n",
    "                print(f\"Data looks good: found {df['Player'].notna().sum()} players with names\")\n",
    "            else:\n",
    "                print(f\"Warning: Player column is missing or empty in {table_name}\")\n",
    "        \n",
    "        # Add a row number column as a fallback for player identification\n",
    "        df['row_id'] = range(len(df))\n",
    "        \n",
    "        # Store the dataframe\n",
    "        raw_data[table_name] = df\n",
    "        print(f\"loaded {table_name} data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"error loading {table_name} data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69f2f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard table columns:\n",
      "['Rk', 'Player', 'Nation', 'Pos', 'Squad', 'Age', 'Born', 'MP', 'Starts', 'Min', '90s', 'Gls', 'Ast', 'G+A', 'G-PK', 'PK', 'PKatt', 'CrdY', 'CrdR', 'xG', 'npxG', 'xAG', 'npxG+xAG', 'PrgC', 'PrgP', 'PrgR', 'Gls.1', 'Ast.1', 'G+A.1', 'G-PK.1', 'G+A-PK', 'xG.1', 'xAG.1', 'xG+xAG', 'npxG.1', 'npxG+xAG.1', 'row_id']\n",
      "\n",
      "first few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rk</th>\n",
       "      <th>Player</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Squad</th>\n",
       "      <th>Age</th>\n",
       "      <th>Born</th>\n",
       "      <th>MP</th>\n",
       "      <th>Starts</th>\n",
       "      <th>Min</th>\n",
       "      <th>90s</th>\n",
       "      <th>Gls</th>\n",
       "      <th>Ast</th>\n",
       "      <th>G+A</th>\n",
       "      <th>G-PK</th>\n",
       "      <th>PK</th>\n",
       "      <th>PKatt</th>\n",
       "      <th>CrdY</th>\n",
       "      <th>CrdR</th>\n",
       "      <th>xG</th>\n",
       "      <th>npxG</th>\n",
       "      <th>xAG</th>\n",
       "      <th>npxG+xAG</th>\n",
       "      <th>PrgC</th>\n",
       "      <th>PrgP</th>\n",
       "      <th>PrgR</th>\n",
       "      <th>Gls.1</th>\n",
       "      <th>Ast.1</th>\n",
       "      <th>G+A.1</th>\n",
       "      <th>G-PK.1</th>\n",
       "      <th>G+A-PK</th>\n",
       "      <th>xG.1</th>\n",
       "      <th>xAG.1</th>\n",
       "      <th>xG+xAG</th>\n",
       "      <th>npxG.1</th>\n",
       "      <th>npxG+xAG.1</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Max Aarons</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>DF</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>24</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Joshua Acheampong</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>DF</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>18</td>\n",
       "      <td>2006</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tyler Adams</td>\n",
       "      <td>us USA</td>\n",
       "      <td>MF</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>25</td>\n",
       "      <td>1999</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>1,965</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>14</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tosin Adarabioyo</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>DF</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>26</td>\n",
       "      <td>1997</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>1,409</td>\n",
       "      <td>15.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Simon Adingra</td>\n",
       "      <td>ci CIV</td>\n",
       "      <td>FW,MF</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>22</td>\n",
       "      <td>2002</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>1,097</td>\n",
       "      <td>12.2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>136</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rk             Player   Nation    Pos        Squad Age  Born  MP Starts    Min   90s Gls Ast G+A G-PK PK PKatt CrdY CrdR   xG npxG  xAG npxG+xAG PrgC PrgP PrgR Gls.1 Ast.1 G+A.1 G-PK.1 G+A-PK  xG.1 xAG.1 xG+xAG npxG.1 npxG+xAG.1  row_id\n",
       "0  1         Max Aarons  eng ENG     DF  Bournemouth  24  2000   3      1     86     1   0   0   0    0  0     0    0    0    0    0    0        0    1    8    3     0     0     0      0      0     0     0      0      0          0       0\n",
       "1  2  Joshua Acheampong  eng ENG     DF      Chelsea  18  2006   4      2    170   1.9   0   0   0    0  0     0    1    0  0.2  0.2    0      0.2    0    8    0     0     0     0      0      0  0.12     0   0.12   0.12       0.12       1\n",
       "2  3        Tyler Adams   us USA     MF  Bournemouth  25  1999  28     21  1,965  21.8   0   3   3    0  0     0    7    0  1.6  1.6    1      2.6   14   76   10     0  0.14  0.14      0   0.14  0.07  0.05   0.12   0.07       0.12       2\n",
       "3  4   Tosin Adarabioyo  eng ENG     DF      Chelsea  26  1997  22     15  1,409  15.7   1   1   2    1  0     0    4    0  0.9  0.9  0.2      1.2    5   42    1  0.06  0.06  0.13   0.06   0.13  0.06  0.01   0.07   0.06       0.07       3\n",
       "4  5      Simon Adingra   ci CIV  FW,MF     Brighton  22  2002  29     12  1,097  12.2   2   2   4    2  0     0    0    0  2.5  2.5  2.5      4.9   50   18  136  0.16  0.16  0.33   0.16   0.33   0.2   0.2    0.4    0.2        0.4       4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examine the standard table which should have player info\n",
    "if 'standard' in raw_data:\n",
    "    print(\"standard table columns:\")\n",
    "    print(raw_data['standard'].columns.tolist())\n",
    "    print(\"\\nfirst few rows:\")\n",
    "    display(raw_data['standard'].head())\n",
    "else:\n",
    "    print(\"standard table not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddda261",
   "metadata": {},
   "source": [
    "## 5. Clean and Standardize Column Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff466baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned standard table columns:\n",
      "['rk', 'player', 'nation', 'pos', 'squad', 'age', 'born', 'mp', 'starts', 'min', '90s', 'gls', 'ast', 'gplusa', 'g_pk', 'pk', 'pkatt', 'crdy', 'crdr', 'xg', 'npxg', 'xag', 'npxgplusxag', 'prgc', 'prgp', 'prgr', 'gls1', 'ast1', 'gplusa1', 'g_pk1', 'gplusa_pk', 'xg1', 'xag1', 'xgplusxag', 'npxg1', 'npxgplusxag1', 'row_id']\n"
     ]
    }
   ],
   "source": [
    "# clean column names for all dataframes\n",
    "cleaned_data = {}\n",
    "for table_name, df in raw_data.items():\n",
    "    cleaned_data[table_name] = clean_column_names(df)\n",
    "    \n",
    "# display the cleaned column names for the standard table\n",
    "if 'standard' in cleaned_data:\n",
    "    print(\"cleaned standard table columns:\")\n",
    "    print(cleaned_data['standard'].columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f84716a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misc key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "passing key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "playtime key columns: {'player': 'player', 'minutes': 'min', 'position': 'pos', 'team': 'squad'}\n",
      "defense key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "standard key columns: {'player': 'player', 'minutes': 'min', 'position': 'pos', 'team': 'squad'}\n",
      "pass_types key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "creation key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "possession key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n",
      "shooting key columns: {'player': 'player', 'minutes': None, 'position': 'pos', 'team': 'squad'}\n"
     ]
    }
   ],
   "source": [
    "# identify key columns in each dataframe\n",
    "key_columns = {}\n",
    "for table_name, df in cleaned_data.items():\n",
    "    key_columns[table_name] = {\n",
    "        'player': identify_player_column(df),\n",
    "        'minutes': identify_minutes_column(df),\n",
    "        'position': identify_position_column(df),\n",
    "        'team': identify_team_column(df)\n",
    "    }\n",
    "    print(f\"{table_name} key columns: {key_columns[table_name]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7ecb857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column mapping:\n",
      "{'pos': 'position', 'squad': 'team', 'min': 'minutes'}\n"
     ]
    }
   ],
   "source": [
    "# create a column mapping dictionary to standardize column names across tables\n",
    "column_mapping = {}\n",
    "for table_name, columns in key_columns.items():\n",
    "    for key, value in columns.items():\n",
    "        if value is not None and value != key:\n",
    "            column_mapping[value] = key\n",
    "\n",
    "print(\"column mapping:\")\n",
    "print(column_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84e380ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized standard table columns:\n",
      "['player', 'position', 'team', 'minutes']\n"
     ]
    }
   ],
   "source": [
    "# standardize column names across all tables\n",
    "standardized_data = {}\n",
    "for table_name, df in cleaned_data.items():\n",
    "    standardized_data[table_name] = standardize_column_names(df, column_mapping)\n",
    "    \n",
    "# verify standardization for the standard table\n",
    "if 'standard' in standardized_data:\n",
    "    print(\"standardized standard table columns:\")\n",
    "    print([col for col in standardized_data['standard'].columns if col in REQUIRED_COLUMNS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0caf12f",
   "metadata": {},
   "source": [
    "## 6. Filter Players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "93e9b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered outfield players: 552 remaining\n",
      "\n",
      "position distribution:\n",
      "position\n",
      "DF       186\n",
      "MF       112\n",
      "FW        85\n",
      "FW,MF     60\n",
      "MF,FW     44\n",
      "Pos       22\n",
      "DF,MF     16\n",
      "MF,DF     13\n",
      "FW,DF      7\n",
      "DF,FW      7\n",
      "Name: count, dtype: int64\n",
      "filtered by minimum minutes (600): 340 remaining\n"
     ]
    }
   ],
   "source": [
    "# start with the standard table as the base for player filtering\n",
    "if 'standard' in standardized_data:\n",
    "    base_df = standardized_data['standard'].copy()\n",
    "    \n",
    "    # Check if position column has any non-NaN values\n",
    "    if 'position' in base_df.columns and base_df['position'].notna().sum() > 0:\n",
    "        # filter outfield players\n",
    "        base_df = filter_outfield_players(base_df, 'position', EXCLUDED_POSITIONS)\n",
    "        print(f\"filtered outfield players: {base_df.shape[0]} remaining\")\n",
    "        \n",
    "        # display position distribution\n",
    "        position_counts = base_df['position'].value_counts()\n",
    "        print(\"\\nposition distribution:\")\n",
    "        if len(position_counts) > 0:\n",
    "            print(position_counts)\n",
    "        else:\n",
    "            print(\"No position data available - all values are NaN\")\n",
    "    else:\n",
    "        print(\"Position column is empty or missing\")\n",
    "    \n",
    "    # filter by minimum minutes\n",
    "    if 'minutes' in base_df.columns:\n",
    "        base_df = filter_by_minutes(base_df, 'minutes', MIN_MINUTES)\n",
    "        print(f\"filtered by minimum minutes ({MIN_MINUTES}): {base_df.shape[0]} remaining\")\n",
    "else:\n",
    "    print(\"standard table not found, cannot filter players\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68ca22",
   "metadata": {},
   "source": [
    "## 7. Aggregate Player Data Across Teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bce916ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified player column: player\n",
      "Identified team column: team\n",
      "found 12 players who played for multiple teams:\n",
      "- Axel Disasi: Aston Villa, Chelsea\n",
      "- Carlos Alcaraz: Southampton, Everton\n",
      "- Evan Ferguson: West Ham, Brighton\n",
      "- Jaden Philogene Bidace: Aston Villa, Ipswich Town\n",
      "- James Ward-Prowse: West Ham, Nott'ham Forest\n",
      "- Joachim Andersen: Crystal Palace, Fulham\n",
      "- Jordan Ayew: Crystal Palace, Leicester City\n",
      "- Julio Enciso: Ipswich Town, Brighton\n",
      "- Marcus Rashford: Manchester Utd, Aston Villa\n",
      "- Odsonne Édouard: Crystal Palace, Leicester City\n",
      "- Reiss Nelson: Arsenal, Fulham\n",
      "- Trevoh Chalobah: Crystal Palace, Chelsea\n"
     ]
    }
   ],
   "source": [
    "# check if there are players who played for multiple teams\n",
    "if 'standard' in standardized_data:\n",
    "    player_col = identify_player_column(standardized_data['standard'])\n",
    "    team_col = identify_team_column(standardized_data['standard'])\n",
    "    \n",
    "    print(f\"Identified player column: {player_col}\")\n",
    "    print(f\"Identified team column: {team_col}\")\n",
    "    \n",
    "    if player_col and team_col and player_col in standardized_data['standard'].columns and team_col in standardized_data['standard'].columns:\n",
    "        # Check if both columns have non-NaN values\n",
    "        if standardized_data['standard'][player_col].notna().sum() > 0 and standardized_data['standard'][team_col].notna().sum() > 0:\n",
    "            # Check for duplicate players across teams\n",
    "            try:\n",
    "                player_team_counts = standardized_data['standard'].groupby(player_col)[team_col].nunique()\n",
    "                multi_team_players = player_team_counts[player_team_counts > 1]\n",
    "                \n",
    "                print(f\"found {len(multi_team_players)} players who played for multiple teams:\")\n",
    "                if len(multi_team_players) > 0:\n",
    "                    for player in multi_team_players.index:\n",
    "                        teams = standardized_data['standard'][standardized_data['standard'][player_col] == player][team_col].unique()\n",
    "                        print(f\"- {player}: {', '.join(teams)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking for multi-team players: {e}\")\n",
    "                print(\"Continuing with data processing...\")\n",
    "        else:\n",
    "            print(f\"Player or team column contains only NaN values\")\n",
    "    else:\n",
    "        print(f\"Missing player or team column in standard data.\")\n",
    "        print(f\"Available columns: {standardized_data['standard'].columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b671cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated misc data: 596 rows → 563 rows\n",
      "aggregated passing data: 596 rows → 563 rows\n",
      "aggregated playtime data: 730 rows → 686 rows\n",
      "aggregated defense data: 596 rows → 563 rows\n",
      "aggregated standard data: 596 rows → 563 rows\n",
      "aggregated pass_types data: 596 rows → 563 rows\n",
      "aggregated creation data: 596 rows → 563 rows\n",
      "aggregated possession data: 596 rows → 563 rows\n",
      "aggregated shooting data: 596 rows → 563 rows\n"
     ]
    }
   ],
   "source": [
    "# function to aggregate player data across teams\n",
    "def aggregate_player_data(df: pd.DataFrame, player_col: str = 'player') -> pd.DataFrame:\n",
    "    \"\"\"aggregate player statistics across multiple teams\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Fix for empty player values - if any player values are NaN, we can't aggregate properly\n",
    "    if player_col in df_copy.columns and df_copy[player_col].isna().any():\n",
    "        # Fill NaN player values with a placeholder based on row index\n",
    "        df_copy[player_col] = df_copy[player_col].fillna(df_copy.index.astype(str))\n",
    "    \n",
    "    # If player_col is not in the dataframe, use row_id as a fallback\n",
    "    if player_col not in df_copy.columns and 'row_id' in df_copy.columns:\n",
    "        player_col = 'row_id'\n",
    "    \n",
    "    # If we still don't have a valid player column, return the original dataframe\n",
    "    if player_col not in df_copy.columns:\n",
    "        return df_copy\n",
    "    \n",
    "    # Check if there are any duplicates to aggregate\n",
    "    if df_copy[player_col].duplicated().sum() == 0:\n",
    "        return df_copy\n",
    "    \n",
    "    # identify numeric columns for summation\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # identify categorical columns to keep the most frequent value\n",
    "    categorical_cols = [col for col in df_copy.columns if col not in numeric_cols and col != player_col]\n",
    "    \n",
    "    # create aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        if col != player_col:  # Ensure we don't include player_col in agg_dict\n",
    "            agg_dict[col] = 'sum'\n",
    "            \n",
    "    for col in categorical_cols:\n",
    "        if col != player_col:  # Ensure we don't include player_col in agg_dict\n",
    "            agg_dict[col] = lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else None\n",
    "    \n",
    "    # If no columns to aggregate, return the original dataframe\n",
    "    if not agg_dict:\n",
    "        return df_copy\n",
    "    \n",
    "    # perform aggregation without resetting index immediately\n",
    "    result = df_copy.groupby(player_col).agg(agg_dict)\n",
    "    \n",
    "    # Reset index to get player_col back as a column\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# aggregate player data for each table\n",
    "aggregated_data = {}\n",
    "for table_name, df in standardized_data.items():\n",
    "    # Identify the player column for this table\n",
    "    player_col = identify_player_column(df)\n",
    "    \n",
    "    if player_col:\n",
    "        try:\n",
    "            aggregated_data[table_name] = aggregate_player_data(df, player_col)\n",
    "            print(f\"aggregated {table_name} data: {df.shape[0]} rows → {aggregated_data[table_name].shape[0]} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"error aggregating {table_name} data: {e}\")\n",
    "            # Use the original data as fallback\n",
    "            aggregated_data[table_name] = df\n",
    "    else:\n",
    "        print(f\"skipping {table_name} data: no player column identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ecc83",
   "metadata": {},
   "source": [
    "## 8. Merge All Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "470c935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged with misc data: now 61 columns\n",
      "merged with passing data: now 92 columns\n",
      "merged with playtime data: now 121 columns\n",
      "merged with defense data: now 145 columns\n",
      "merged with pass_types data: now 168 columns\n",
      "merged with creation data: now 192 columns\n",
      "merged with possession data: now 222 columns\n",
      "merged with shooting data: now 247 columns\n"
     ]
    }
   ],
   "source": [
    "# start with the standard table as the base\n",
    "if 'standard' in aggregated_data and len(aggregated_data['standard']) > 0:\n",
    "    merged_df = aggregated_data['standard'].copy()\n",
    "    \n",
    "    # Identify the player column in the standard table\n",
    "    base_player_col = identify_player_column(merged_df)\n",
    "    \n",
    "    if base_player_col:\n",
    "        # merge with other tables\n",
    "        for table_name, df in aggregated_data.items():\n",
    "            if table_name != 'standard' and len(df) > 0:\n",
    "                # Identify the player column in this table\n",
    "                table_player_col = identify_player_column(df)\n",
    "                \n",
    "                if table_player_col:\n",
    "                    # identify columns to merge (exclude player column to avoid duplicates)\n",
    "                    merge_cols = [col for col in df.columns if col != table_player_col]\n",
    "                    \n",
    "                    # Make sure we have at least one column to merge\n",
    "                    if len(merge_cols) > 0:\n",
    "                        try:\n",
    "                            # merge with the base dataframe\n",
    "                            merged_df = pd.merge(\n",
    "                                merged_df,\n",
    "                                df[[table_player_col] + merge_cols],\n",
    "                                left_on=base_player_col,\n",
    "                                right_on=table_player_col,\n",
    "                                how='left',\n",
    "                                suffixes=('', f'_{table_name}')\n",
    "                            )\n",
    "                            \n",
    "                            # If the right player column was added with a suffix, drop it\n",
    "                            if table_player_col != base_player_col and f\"{table_player_col}_{table_name}\" in merged_df.columns:\n",
    "                                merged_df = merged_df.drop(f\"{table_player_col}_{table_name}\", axis=1)\n",
    "                            \n",
    "                            print(f\"merged with {table_name} data: now {merged_df.shape[1]} columns\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"error merging {table_name} data: {e}\")\n",
    "                else:\n",
    "                    print(f\"skipping {table_name} data: no player column identified\")\n",
    "        \n",
    "        # check for duplicate column names after merging\n",
    "        duplicate_cols = merged_df.columns[merged_df.columns.duplicated()].tolist()\n",
    "        if duplicate_cols:\n",
    "            print(f\"warning: found duplicate column names: {duplicate_cols}\")\n",
    "    else:\n",
    "        print(\"No player column found in standard table, cannot merge tables\")\n",
    "else:\n",
    "    print(\"Standard table not found or is empty, cannot merge tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ed1ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle duplicate columns if any\n",
    "if 'merged_df' in locals() and duplicate_cols:\n",
    "    # drop duplicate columns\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(f\"dropped duplicate columns: now {merged_df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30f3470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final dataset: 563 rows, 248 columns\n"
     ]
    }
   ],
   "source": [
    "# add source table information\n",
    "if 'merged_df' in locals() and len(merged_df) > 0:\n",
    "    # create a dictionary to track which columns came from which table\n",
    "    column_sources = {}\n",
    "    for table_name, df in standardized_data.items():\n",
    "        for col in df.columns:\n",
    "            # handle potential suffixes from merging\n",
    "            if col in merged_df.columns:\n",
    "                column_sources[col] = table_name\n",
    "            elif f\"{col}_{table_name}\" in merged_df.columns:\n",
    "                column_sources[f\"{col}_{table_name}\"] = table_name\n",
    "    \n",
    "    # add a source_table column\n",
    "    merged_df['data_source'] = 'fbref_2024_25'\n",
    "    \n",
    "    # Make sure we have the required columns\n",
    "    for col in REQUIRED_COLUMNS:\n",
    "        if col not in merged_df.columns:\n",
    "            # If 'team' is missing but we have 'squad', rename it\n",
    "            if col == 'team' and 'squad' in merged_df.columns:\n",
    "                merged_df['team'] = merged_df['squad']\n",
    "            # If 'squad' is missing but we have 'team', rename it\n",
    "            elif col == 'squad' and 'team' in merged_df.columns:\n",
    "                merged_df['squad'] = merged_df['team']\n",
    "    \n",
    "    print(f\"final dataset: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"merged dataframe not created or is empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08686ef",
   "metadata": {},
   "source": [
    "## 9. Validate Final Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8f3e7f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema validation: passed\n",
      "column 'player': 0 missing values (0.00%)\n",
      "column 'team': 0 missing values (0.00%)\n",
      "column 'position': 0 missing values (0.00%)\n",
      "column 'minutes': 0 missing values (0.00%)\n",
      "\n",
      "sample of the final dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>position</th>\n",
       "      <th>minutes</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Cresswell</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>DF</td>\n",
       "      <td>824</td>\n",
       "      <td>fbref_2024_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Ramsdale</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>GK</td>\n",
       "      <td>2,700</td>\n",
       "      <td>fbref_2024_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Wan-Bissaka</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>DF</td>\n",
       "      <td>3,154</td>\n",
       "      <td>fbref_2024_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abdoulaye Doucouré</td>\n",
       "      <td>Everton</td>\n",
       "      <td>MF</td>\n",
       "      <td>2,564</td>\n",
       "      <td>fbref_2024_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abdukodir Khusanov</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>DF</td>\n",
       "      <td>503</td>\n",
       "      <td>fbref_2024_25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               player             team position minutes    data_source\n",
       "0     Aaron Cresswell         West Ham       DF     824  fbref_2024_25\n",
       "1      Aaron Ramsdale      Southampton       GK   2,700  fbref_2024_25\n",
       "2   Aaron Wan-Bissaka         West Ham       DF   3,154  fbref_2024_25\n",
       "3  Abdoulaye Doucouré          Everton       MF   2,564  fbref_2024_25\n",
       "4  Abdukodir Khusanov  Manchester City       DF     503  fbref_2024_25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validate the schema of the final dataset\n",
    "if 'merged_df' in locals():\n",
    "    is_valid = validate_schema(merged_df, REQUIRED_COLUMNS)\n",
    "    print(f\"schema validation: {'passed' if is_valid else 'failed'}\")\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    for col in REQUIRED_COLUMNS:\n",
    "        if col in merged_df.columns:\n",
    "            missing_count = merged_df[col].isna().sum()\n",
    "            print(f\"column '{col}': {missing_count} missing values ({missing_count/len(merged_df)*100:.2f}%)\")\n",
    "    \n",
    "    # display sample of the final dataset\n",
    "    print(\"\\nsample of the final dataset:\")\n",
    "    display(merged_df[REQUIRED_COLUMNS + ['data_source']].head())\n",
    "else:\n",
    "    print(\"merged dataframe not created, cannot validate schema\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9124b",
   "metadata": {},
   "source": [
    "## 10. Save Cleaned Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35dd5590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved cleaned dataset to ../data/processed/player_stats_cleaned.csv\n",
      "saved schema information to ../data/processed/data_schema.json\n"
     ]
    }
   ],
   "source": [
    "# ensure the output directory exists\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# save the cleaned dataset\n",
    "if 'merged_df' in locals() and is_valid:\n",
    "    merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"saved cleaned dataset to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # create and save schema information\n",
    "    schema_info = {\n",
    "        'num_rows': len(merged_df),\n",
    "        'num_columns': len(merged_df.columns),\n",
    "        'columns': {}\n",
    "    }\n",
    "    \n",
    "    for col in merged_df.columns:\n",
    "        schema_info['columns'][col] = {\n",
    "            'dtype': str(merged_df[col].dtype),\n",
    "            'source_table': column_sources.get(col, 'unknown'),\n",
    "            'missing_values': int(merged_df[col].isna().sum()),\n",
    "            'unique_values': int(merged_df[col].nunique()) if merged_df[col].dtype != 'object' or merged_df[col].nunique() < 100 else 'too many to list'\n",
    "        }\n",
    "    \n",
    "    with open(SCHEMA_FILE, 'w') as f:\n",
    "        json.dump(schema_info, f, indent=2)\n",
    "    \n",
    "    print(f\"saved schema information to {SCHEMA_FILE}\")\n",
    "else:\n",
    "    print(\"dataset not saved due to validation failure or missing dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e9a55",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1faad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Summary:\n",
      "- Loaded 9 raw data tables\n",
      "- Processed 563 unique players\n",
      "- Final dataset has 248 columns\n",
      "- Excluded 1 position types: ['GK']\n",
      "- Minimum minutes threshold: 600\n",
      "\n",
      "Next Steps (Phase 2):\n",
      "- Feature Engineering\n",
      "- Per-90 normalization\n",
      "- Composite indices calculation (PI, CCI, DA, FE)\n",
      "- Winsorization at 5th and 95th percentiles\n"
     ]
    }
   ],
   "source": [
    "# print summary statistics\n",
    "if 'merged_df' in locals():\n",
    "    print(\"Data Cleaning Summary:\")\n",
    "    print(f\"- Loaded {len(raw_data)} raw data tables\")\n",
    "    print(f\"- Processed {merged_df.shape[0]} unique players\")\n",
    "    print(f\"- Final dataset has {merged_df.shape[1]} columns\")\n",
    "    print(f\"- Excluded {len(EXCLUDED_POSITIONS)} position types: {EXCLUDED_POSITIONS}\")\n",
    "    print(f\"- Minimum minutes threshold: {MIN_MINUTES}\")\n",
    "    \n",
    "    # print next steps\n",
    "    print(\"\\nNext Steps (Phase 2):\")\n",
    "    print(\"- Feature Engineering\")\n",
    "    print(\"- Per-90 normalization\")\n",
    "    print(\"- Composite indices calculation (PI, CCI, DA, FE)\")\n",
    "    print(\"- Winsorization at 5th and 95th percentiles\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
